[2019-02-19 20:22:37,250] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 20:22:37,257] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:22:37,258] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:22:37,258] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:22:37,259] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-02-19 20:22:37,272] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 20:22:37,273] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-02-19 20:22:41,813] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,814] INFO Server environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,814] INFO Server environment:java.version=1.8.0_201 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,814] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,814] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,815] INFO Server environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,816] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,817] INFO Server environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,819] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,831] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,833] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,834] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,835] INFO Server environment:user.name=Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,835] INFO Server environment:user.home=C:\Users\Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,836] INFO Server environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,872] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,872] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,874] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:41,915] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-02-19 20:22:41,919] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:22:49,703] INFO Expiring session 0x10006c52e520001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:49,704] INFO Expiring session 0x10006c52e520000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:22:49,706] INFO Processed session termination for sessionid: 0x10006c52e520001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:22:49,706] INFO Processed session termination for sessionid: 0x10006c52e520000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:22:49,706] INFO Creating new log file: log.de (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-02-19 20:24:13,919] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:24:14,430] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:24:14,431] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:24:14,450] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:16,979] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:24:17,489] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:24:17,491] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:24:17,510] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:18,981] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,982] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,982] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,982] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,983] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,983] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,985] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,985] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,985] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,986] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,987] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,988] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,988] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,989] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,989] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:18,993] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:19,065] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:19,066] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:19,069] INFO Accepted socket connection from /127.0.0.1:58598 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:24:19,069] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:19,118] INFO Client attempting to establish new session at /127.0.0.1:58598 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:19,136] INFO Established session 0x1000a8f6f890000 with negotiated timeout 6000 for client /127.0.0.1:58598 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:19,138] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000a8f6f890000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:19,144] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:19,318] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x1 zxid:0xe1 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,353] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x2 zxid:0xe2 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,377] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x3 zxid:0xe3 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,391] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x4 zxid:0xe4 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,410] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x5 zxid:0xe5 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,424] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x6 zxid:0xe6 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,443] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x7 zxid:0xe7 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,458] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x8 zxid:0xe8 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,477] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0x9 zxid:0xe9 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,491] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0xa zxid:0xea txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,510] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0xb zxid:0xeb txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,524] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0xc zxid:0xec txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,543] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:create cxid:0xd zxid:0xed txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:19,807] INFO Cluster ID = GgNQQO0OSsCxcUz7n4EETw (kafka.server.KafkaServer)
[2019-02-19 20:24:19,954] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:19,973] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:20,026] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:20,041] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:20,026] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:20,101] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:24:20,137] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:24:20,379] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,382] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,514] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,517] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 246 ms (kafka.log.Log)
[2019-02-19 20:24:20,539] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,540] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,586] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,587] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-02-19 20:24:20,603] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,603] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,622] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,623] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[2019-02-19 20:24:20,637] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,637] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,649] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:24:20,649] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,650] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:24:20,650] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2019-02-19 20:24:20,669] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,669] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,670] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:20,686] INFO [ProducerStateManager partition=test-cluster-topic1-0] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,738] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,741] INFO [ProducerStateManager partition=test-cluster-topic1-0] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\test-cluster-topic1-0\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,759] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 101 ms (kafka.log.Log)
[2019-02-19 20:24:20,785] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,785] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,795] INFO [ProducerStateManager partition=test-cluster-topic1-2] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,829] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,832] INFO [ProducerStateManager partition=test-cluster-topic1-2] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\test-cluster-topic1-2\00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,833] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 58 ms (kafka.log.Log)
[2019-02-19 20:24:20,859] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,860] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,873] INFO [ProducerStateManager partition=test-cluster-topic1-3] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,928] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:20,931] INFO [ProducerStateManager partition=test-cluster-topic1-3] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\test-cluster-topic1-3\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:20,931] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 90 ms (kafka.log.Log)
[2019-02-19 20:24:20,961] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:20,963] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,030] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,033] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 93 ms (kafka.log.Log)
[2019-02-19 20:24:21,077] WARN [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Found a corrupted index file corresponding to log file D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.log due to Corrupt time index found, time index file (D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1550576941500}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-02-19 20:24:21,079] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,104] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 114 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:21,106] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,106] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,128] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 114 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:21,188] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 114 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,190] INFO [ProducerStateManager partition=__consumer_offsets-12] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000114.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:21,191] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 114 in 150 ms (kafka.log.Log)
[2019-02-19 20:24:21,218] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,219] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,262] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,264] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 66 ms (kafka.log.Log)
[2019-02-19 20:24:21,281] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,281] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,321] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,323] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:24:21,340] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,341] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,388] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,389] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 62 ms (kafka.log.Log)
[2019-02-19 20:24:21,407] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,407] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,445] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,447] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:21,468] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,469] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,512] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,513] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 58 ms (kafka.log.Log)
[2019-02-19 20:24:21,528] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,528] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,578] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,579] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 62 ms (kafka.log.Log)
[2019-02-19 20:24:21,593] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,594] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,629] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,630] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:24:21,645] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,646] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,686] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,687] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 20:24:21,703] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,703] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,736] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,737] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:24:21,751] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,752] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,786] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,787] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:24:21,807] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,807] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,845] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,846] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-02-19 20:24:21,872] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,873] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,912] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,914] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 62 ms (kafka.log.Log)
[2019-02-19 20:24:21,942] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:21,942] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,986] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:21,987] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 62 ms (kafka.log.Log)
[2019-02-19 20:24:22,003] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:22,003] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,042] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,042] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,043] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,043] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,043] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,043] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,044] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,044] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,045] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,045] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,045] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,046] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:24:22,046] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,047] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,048] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,048] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,048] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,050] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:22,066] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:22,066] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,072] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:22,072] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:22,079] INFO Accepted socket connection from /127.0.0.1:58602 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:24:22,079] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:22,084] INFO Client attempting to establish new session at /127.0.0.1:58602 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:22,110] INFO Established session 0x1000a8f6f890001 with negotiated timeout 6000 for client /127.0.0.1:58602 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:22,112] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000a8f6f890001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:22,116] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:22,119] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,120] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 67 ms (kafka.log.Log)
[2019-02-19 20:24:22,124] INFO Logs loading complete in 1985 ms. (kafka.log.LogManager)
[2019-02-19 20:24:22,151] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:24:22,153] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:24:22,166] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x1 zxid:0xef txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,204] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x2 zxid:0xf0 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,228] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x3 zxid:0xf1 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,242] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x4 zxid:0xf2 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,278] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x5 zxid:0xf3 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,303] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x6 zxid:0xf4 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,316] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x7 zxid:0xf5 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,345] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x8 zxid:0xf6 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,378] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0x9 zxid:0xf7 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,393] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0xa zxid:0xf8 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,412] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0xb zxid:0xf9 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,444] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0xc zxid:0xfa txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,469] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890001 type:create cxid:0xd zxid:0xfb txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:22,660] INFO Awaiting socket connections on 0.0.0.0:9091. (kafka.network.Acceptor)
[2019-02-19 20:24:22,672] INFO Cluster ID = GgNQQO0OSsCxcUz7n4EETw (kafka.server.KafkaServer)
[2019-02-19 20:24:22,747] INFO [SocketServer brokerId=1] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:24:22,778] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:22,778] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:22,778] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:22,789] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:22,800] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:24:22,807] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:22,833] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:22,833] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:22,835] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:22,870] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:24:22,940] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:22,943] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,985] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:22,988] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-02-19 20:24:23,007] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,007] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,017] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,018] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[2019-02-19 20:24:23,033] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,033] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,090] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,091] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 68 ms (kafka.log.Log)
[2019-02-19 20:24:23,106] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,107] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,145] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,146] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-02-19 20:24:23,162] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,162] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,179] INFO [ProducerStateManager partition=test-cluster-topic1-0] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:23,247] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka2-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,251] INFO [ProducerStateManager partition=test-cluster-topic1-0] Loading producer state from snapshot file 'D:\tmp\kafka2-logs\test-cluster-topic1-0\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:23,260] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 109 ms (kafka.log.Log)
[2019-02-19 20:24:23,278] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,279] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,290] INFO [ProducerStateManager partition=test-cluster-topic1-1] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:23,322] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,323] INFO [ProducerStateManager partition=test-cluster-topic1-1] Loading producer state from snapshot file 'D:\tmp\kafka2-logs\test-cluster-topic1-1\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:23,324] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 57 ms (kafka.log.Log)
[2019-02-19 20:24:23,339] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,339] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,379] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,381] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 20:24:23,419] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,420] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,471] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,474] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 73 ms (kafka.log.Log)
[2019-02-19 20:24:23,501] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,501] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,546] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,547] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-02-19 20:24:23,564] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,565] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,605] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,607] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:24:23,625] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,626] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,664] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,665] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 20:24:23,683] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,683] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,722] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,725] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:24:23,752] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,753] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,831] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,833] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 100 ms (kafka.log.Log)
[2019-02-19 20:24:23,859] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,860] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,968] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:23,970] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 129 ms (kafka.log.Log)
[2019-02-19 20:24:23,997] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:23,998] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,056] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,058] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-02-19 20:24:24,093] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,094] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,140] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,142] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 68 ms (kafka.log.Log)
[2019-02-19 20:24:24,168] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,168] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,213] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,215] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2019-02-19 20:24:24,241] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,241] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,278] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,280] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-02-19 20:24:24,296] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,296] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,337] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,338] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 20:24:24,353] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,354] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,395] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,396] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 20:24:24,423] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,424] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,444] INFO [ProducerStateManager partition=__consumer_offsets-46] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:24,472] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka2-logs] Loading producer state till offset 1 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,473] INFO [ProducerStateManager partition=__consumer_offsets-46] Loading producer state from snapshot file 'D:\tmp\kafka2-logs\__consumer_offsets-46\00000000000000000001.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:24,473] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 1 in 70 ms (kafka.log.Log)
[2019-02-19 20:24:24,489] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,490] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,529] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,530] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 20:24:24,545] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka2-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:24,545] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,579] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka2-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:24,581] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka2-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:24:24,588] INFO Logs loading complete in 1717 ms. (kafka.log.LogManager)
[2019-02-19 20:24:24,609] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:24:24,611] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:24:24,835] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 20:24:24,865] INFO [SocketServer brokerId=2] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:24:24,900] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:24,901] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:24,901] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:24,913] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:24:25,205] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,206] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,206] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,206] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,207] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,207] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,208] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,209] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,209] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,210] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,211] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,211] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,212] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,213] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,213] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,217] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:24:25,253] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:25,253] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:25,257] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:58622 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:24:25,257] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:25,262] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:58622 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:25,286] INFO Established session 0x1000a8f6f890002 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:58622 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:24:25,289] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000a8f6f890002, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:24:25,293] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:24:25,344] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x1 zxid:0xfd txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,381] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x2 zxid:0xfe txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,393] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x3 zxid:0xff txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,412] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x4 zxid:0x100 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,427] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x5 zxid:0x101 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,445] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x6 zxid:0x102 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,461] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x7 zxid:0x103 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,479] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x8 zxid:0x104 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,494] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0x9 zxid:0x105 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,513] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0xa zxid:0x106 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,527] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0xb zxid:0x107 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,547] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0xc zxid:0x108 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,560] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890002 type:create cxid:0xd zxid:0x109 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:25,756] INFO Cluster ID = GgNQQO0OSsCxcUz7n4EETw (kafka.server.KafkaServer)
[2019-02-19 20:24:25,836] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:25,846] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:24:25,874] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:25,874] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:25,878] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:24:25,915] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:24:25,988] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:25,990] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,065] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,067] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 114 ms (kafka.log.Log)
[2019-02-19 20:24:26,086] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,087] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,098] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,099] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-02-19 20:24:26,113] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,113] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,125] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,126] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[2019-02-19 20:24:26,140] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,141] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,180] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,181] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[2019-02-19 20:24:26,195] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,196] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,212] INFO [ProducerStateManager partition=test-cluster-topic1-1] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,249] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka3-logs] Loading producer state till offset 1 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,252] INFO [ProducerStateManager partition=test-cluster-topic1-1] Loading producer state from snapshot file 'D:\tmp\kafka3-logs\test-cluster-topic1-1\00000000000000000001.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,260] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 1 in 74 ms (kafka.log.Log)
[2019-02-19 20:24:26,275] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,276] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,286] INFO [ProducerStateManager partition=test-cluster-topic1-2] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,323] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,324] INFO [ProducerStateManager partition=test-cluster-topic1-2] Loading producer state from snapshot file 'D:\tmp\kafka3-logs\test-cluster-topic1-2\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,324] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 60 ms (kafka.log.Log)
[2019-02-19 20:24:26,341] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,342] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,353] INFO [ProducerStateManager partition=test-cluster-topic1-3] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,381] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka3-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,383] INFO [ProducerStateManager partition=test-cluster-topic1-3] Loading producer state from snapshot file 'D:\tmp\kafka3-logs\test-cluster-topic1-3\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:26,383] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:26,404] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,405] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,446] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,447] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:26,463] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,463] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,497] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,498] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:24:26,513] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,513] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,555] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,556] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:26,582] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,583] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,621] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,622] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-02-19 20:24:26,638] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,638] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,672] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,673] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:24:26,697] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,698] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,746] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,748] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 70 ms (kafka.log.Log)
[2019-02-19 20:24:26,766] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,767] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,805] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,807] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:26,824] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,825] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,863] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,864] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:26,880] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,880] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,914] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,915] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:24:26,934] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,935] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,973] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:26,974] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 20:24:26,990] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:26,990] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,031] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,032] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 20:24:27,055] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:27,056] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,115] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,117] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 80 ms (kafka.log.Log)
[2019-02-19 20:24:27,154] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:27,155] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,167] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2019-02-19 20:24:27,198] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka3-logs] Loading producer state till offset 2 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,199] INFO [ProducerStateManager partition=__consumer_offsets-44] Loading producer state from snapshot file 'D:\tmp\kafka3-logs\__consumer_offsets-44\00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:24:27,200] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 67 ms (kafka.log.Log)
[2019-02-19 20:24:27,217] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:27,218] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,265] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,266] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 60 ms (kafka.log.Log)
[2019-02-19 20:24:27,281] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:27,281] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,323] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,324] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 20:24:27,339] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka3-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:24:27,339] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,380] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka3-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:24:27,381] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka3-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:24:27,387] INFO Logs loading complete in 1470 ms. (kafka.log.LogManager)
[2019-02-19 20:24:27,390] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:27,399] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:24:27,400] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:24:27,411] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:27,412] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9091,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:27,492] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,494] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,494] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,543] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:27,545] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:27,556] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 11 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:27,623] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:3000,blockEndProducerId:3999) by writing to Zk with path version 4 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:24:27,661] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:27,668] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:27,676] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:24:27,725] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:24:27,744] INFO [SocketServer brokerId=1] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:24:27,776] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:27,802] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:27,801] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.Acceptor)
[2019-02-19 20:24:27,813] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[2019-02-19 20:24:27,864] INFO [SocketServer brokerId=3] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:24:27,880] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-30, __consumer_offsets-21, __consumer_offsets-27, __consumer_offsets-9, test-cluster-topic1-3, __consumer_offsets-33, test-cluster-topic1-0, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, test-cluster-topic-1, test-cluster-topic1-2, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-48, test-cluster-topic-0, __consumer_offsets-6, __consumer_offsets-0, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:27,912] INFO [ExpirationReaper-3-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,912] INFO [ExpirationReaper-3-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,912] INFO [ExpirationReaper-3-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:27,920] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:27,948] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:multi cxid:0xb8 zxid:0x10f txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:27,956] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:27,959] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:24:28,007] INFO Got user-level KeeperException when processing sessionid:0x1000a8f6f890000 type:multi cxid:0xba zxid:0x110 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:24:28,054] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,055] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,126] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,127] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,158] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,158] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,191] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,192] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,286] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,286] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,319] INFO Replica loaded for partition test-cluster-topic1-2 with initial high watermark 3 (kafka.cluster.Replica)
[2019-02-19 20:24:28,321] INFO Replica loaded for partition test-cluster-topic1-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,321] INFO [Partition test-cluster-topic1-2 broker=1] test-cluster-topic1-2 starts at Leader Epoch 1 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,332] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,333] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,334] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,335] INFO [Partition test-cluster-topic-0 broker=1] test-cluster-topic-0 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,369] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,370] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,445] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,445] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,477] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,478] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,512] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,513] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,569] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,571] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,603] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,604] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,637] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,638] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,640] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,641] INFO [Partition test-cluster-topic-1 broker=1] test-cluster-topic-1 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,703] INFO Replica loaded for partition test-cluster-topic1-3 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:28,703] INFO Replica loaded for partition test-cluster-topic1-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,705] INFO [Partition test-cluster-topic1-3 broker=1] test-cluster-topic1-3 starts at Leader Epoch 1 from offset 2. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,735] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,736] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,769] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:28,769] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,771] INFO [Partition test-cluster-topic1-0 broker=1] test-cluster-topic1-0 starts at Leader Epoch 0 from offset 2. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,776] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 114 (kafka.cluster.Replica)
[2019-02-19 20:24:28,777] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 0 from offset 114. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,781] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,782] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,820] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,821] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,853] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,854] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,912] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,919] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,921] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set() (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:28,935] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,937] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,938] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,939] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,939] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,939] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,939] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,940] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,940] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,940] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,941] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,941] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,941] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,942] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,942] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,942] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,942] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,973] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-cluster-topic-2, test-cluster-topic-3) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:28,974] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,974] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:28,976] INFO [Partition test-cluster-topic-2 broker=1] test-cluster-topic-2 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:28,989] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 52 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,990] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,990] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:28,992] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,025] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:29,026] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:29,027] INFO [Partition test-cluster-topic-3 broker=1] test-cluster-topic-3 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:29,044] INFO [GroupCoordinator 1]: Loading group metadata for test-group with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:29,045] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 52 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,052] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,052] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,052] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,053] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,053] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,054] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,055] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,056] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,057] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,058] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,058] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,489] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:29,514] INFO Result of znode creation at /brokers/ids/2 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:29,516] INFO Registered broker 2 at path /brokers/ids/2 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:29,595] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:29,597] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:29,598] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:29,673] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:29,675] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:29,681] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:29,708] INFO [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:4000,blockEndProducerId:4999) by writing to Zk with path version 5 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:24:29,728] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:29,730] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:24:29,730] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:29,766] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:24:29,784] INFO [SocketServer brokerId=2] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:24:29,789] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:29,790] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:29,791] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
[2019-02-19 20:24:29,865] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-4, __consumer_offsets-7, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-49, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, test-cluster-topic1-1, __consumer_offsets-37, __consumer_offsets-19, __consumer_offsets-13, __consumer_offsets-43, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:29,882] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:29,888] INFO [Partition __consumer_offsets-10 broker=2] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:29,943] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:29,945] INFO [Partition __consumer_offsets-7 broker=2] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:29,976] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:29,976] INFO [Partition __consumer_offsets-4 broker=2] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,012] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,013] INFO [Partition __consumer_offsets-1 broker=2] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,046] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,047] INFO [Partition __consumer_offsets-49 broker=2] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,088] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 1 (kafka.cluster.Replica)
[2019-02-19 20:24:30,089] INFO [Partition __consumer_offsets-46 broker=2] __consumer_offsets-46 starts at Leader Epoch 0 from offset 1. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,096] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,097] INFO [Partition __consumer_offsets-43 broker=2] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,127] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,127] INFO [Partition __consumer_offsets-40 broker=2] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,187] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,187] INFO [Partition __consumer_offsets-37 broker=2] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,221] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,222] INFO [Partition __consumer_offsets-34 broker=2] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,254] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,255] INFO [Partition __consumer_offsets-31 broker=2] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,297] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,300] INFO [Partition __consumer_offsets-19 broker=2] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,363] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,364] INFO [Partition __consumer_offsets-28 broker=2] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,446] INFO Replica loaded for partition test-cluster-topic1-1 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:30,448] INFO Replica loaded for partition test-cluster-topic1-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,449] INFO [Partition test-cluster-topic1-1 broker=2] test-cluster-topic1-1 starts at Leader Epoch 1 from offset 2. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,460] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,461] INFO [Partition __consumer_offsets-16 broker=2] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,489] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,490] INFO [Partition __consumer_offsets-25 broker=2] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,514] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,515] INFO [Partition __consumer_offsets-22 broker=2] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,539] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,540] INFO [Partition __consumer_offsets-13 broker=2] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:30,581] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,583] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,584] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,587] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,597] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,598] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,599] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,600] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,605] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,605] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,610] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,610] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,611] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:30,613] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:30,615] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(test-cluster-topic1-0, test-cluster-topic-2, test-cluster-topic-0, test-cluster-topic-1, test-cluster-topic-3) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:30,642] INFO [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:30,646] INFO [ReplicaFetcherManager on broker 2] Added fetcher to broker BrokerEndPoint(id=1, host=DESKTOP-6IV0LP1:9091) for partitions Map(test-cluster-topic-2 -> (offset=0, leaderEpoch=2), test-cluster-topic1-0 -> (offset=2, leaderEpoch=0), test-cluster-topic-3 -> (offset=0, leaderEpoch=2), test-cluster-topic-1 -> (offset=0, leaderEpoch=1), test-cluster-topic-0 -> (offset=0, leaderEpoch=1)) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:30,657] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,660] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,672] WARN [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-3. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:30,674] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,677] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,677] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,677] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:30,678] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,679] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,679] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,679] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,679] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka2-logs] Truncating to 2 has no effect as the largest offset in the log is 1 (kafka.log.Log)
[2019-02-19 20:24:30,681] WARN [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-0. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:30,681] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:30,684] WARN [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-1. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:30,679] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,690] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:30,683] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-22 in 23 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,691] WARN [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-2. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:30,690] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,692] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,692] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:30,694] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,694] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,695] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,695] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,696] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,696] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,696] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,697] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,697] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-37 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,698] INFO [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,698] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,699] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,719] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-4, __consumer_offsets-7, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-49, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, test-cluster-topic1-1, __consumer_offsets-37, __consumer_offsets-19, __consumer_offsets-13, __consumer_offsets-43, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:30,722] INFO [Partition __consumer_offsets-10 broker=2] __consumer_offsets-10 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,723] WARN [LeaderEpochCache __consumer_offsets-10] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,728] INFO [Partition test-cluster-topic-2 broker=1] Expanding ISR from 1 to 1,2 (kafka.cluster.Partition)
[2019-02-19 20:24:30,732] INFO [GroupCoordinator 2]: Loading group metadata for console-consumer-47013 with generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:30,737] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-46 in 37 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,737] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,742] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,743] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,745] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,745] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,745] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,746] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,746] INFO [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:30,750] INFO [Partition __consumer_offsets-7 broker=2] __consumer_offsets-7 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,751] WARN [LeaderEpochCache __consumer_offsets-7] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,765] INFO [Partition test-cluster-topic-3 broker=1] Expanding ISR from 1 to 1,2 (kafka.cluster.Partition)
[2019-02-19 20:24:30,807] INFO [Partition __consumer_offsets-4 broker=2] __consumer_offsets-4 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,808] WARN [LeaderEpochCache __consumer_offsets-4] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,840] INFO [Partition __consumer_offsets-1 broker=2] __consumer_offsets-1 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,841] WARN [LeaderEpochCache __consumer_offsets-1] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,866] INFO [Partition __consumer_offsets-49 broker=2] __consumer_offsets-49 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,867] WARN [LeaderEpochCache __consumer_offsets-49] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,898] INFO [Partition __consumer_offsets-46 broker=2] __consumer_offsets-46 starts at Leader Epoch 1 from offset 1. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,924] INFO [Partition __consumer_offsets-43 broker=2] __consumer_offsets-43 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,925] WARN [LeaderEpochCache __consumer_offsets-43] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,959] INFO [Partition __consumer_offsets-40 broker=2] __consumer_offsets-40 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,960] WARN [LeaderEpochCache __consumer_offsets-40] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:30,990] INFO [Partition __consumer_offsets-37 broker=2] __consumer_offsets-37 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:30,990] WARN [LeaderEpochCache __consumer_offsets-37] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,023] INFO [Partition __consumer_offsets-34 broker=2] __consumer_offsets-34 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,023] WARN [LeaderEpochCache __consumer_offsets-34] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,048] INFO [Partition __consumer_offsets-31 broker=2] __consumer_offsets-31 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,048] WARN [LeaderEpochCache __consumer_offsets-31] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,073] INFO [Partition __consumer_offsets-19 broker=2] __consumer_offsets-19 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,073] WARN [LeaderEpochCache __consumer_offsets-19] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,099] INFO [Partition __consumer_offsets-28 broker=2] __consumer_offsets-28 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,099] WARN [LeaderEpochCache __consumer_offsets-28] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,124] INFO [Partition __consumer_offsets-25 broker=2] __consumer_offsets-25 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,125] WARN [LeaderEpochCache __consumer_offsets-25] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,150] INFO [Partition test-cluster-topic1-1 broker=2] test-cluster-topic1-1 starts at Leader Epoch 2 from offset 2. Previous Leader Epoch was: 1 (kafka.cluster.Partition)
[2019-02-19 20:24:31,178] INFO [Partition __consumer_offsets-16 broker=2] __consumer_offsets-16 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,178] WARN [LeaderEpochCache __consumer_offsets-16] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,208] INFO [Partition __consumer_offsets-22 broker=2] __consumer_offsets-22 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,208] WARN [LeaderEpochCache __consumer_offsets-22] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:31,293] INFO [Partition __consumer_offsets-13 broker=2] __consumer_offsets-13 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:24:31,293] WARN [LeaderEpochCache __consumer_offsets-13] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:24:32,544] INFO Creating /brokers/ids/3 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:32,565] INFO Result of znode creation at /brokers/ids/3 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:32,568] INFO Registered broker 3 at path /brokers/ids/3 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9093,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:24:32,630] INFO [ExpirationReaper-3-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:32,632] INFO [ExpirationReaper-3-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:32,633] INFO [ExpirationReaper-3-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:24:32,689] INFO [GroupCoordinator 3]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:32,690] INFO [GroupCoordinator 3]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:32,694] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:32,717] INFO [ProducerId Manager 3]: Acquired new producerId block (brokerId:3,blockStartProducerId:5000,blockEndProducerId:5999) by writing to Zk with path version 6 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:24:32,736] INFO [TransactionCoordinator id=3] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:32,738] INFO [TransactionCoordinator id=3] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:24:32,738] INFO [Transaction Marker Channel Manager 3]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:24:32,775] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:24:32,792] INFO [SocketServer brokerId=3] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:24:32,798] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:32,798] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:24:32,800] INFO [KafkaServer id=3] started (kafka.server.KafkaServer)
[2019-02-19 20:24:32,891] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,894] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,897] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,898] INFO Replica loaded for partition test-cluster-topic-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,903] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,905] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,908] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,908] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,909] INFO Replica loaded for partition test-cluster-topic-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,913] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,916] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,918] INFO Replica loaded for partition test-cluster-topic1-2 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:32,918] INFO Replica loaded for partition test-cluster-topic1-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,923] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,924] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,924] INFO Replica loaded for partition test-cluster-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,927] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,930] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,933] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,936] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,939] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,940] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,941] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,943] INFO Replica loaded for partition test-cluster-topic-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,944] INFO Replica loaded for partition test-cluster-topic1-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,946] INFO Replica loaded for partition test-cluster-topic1-3 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:32,948] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,951] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,953] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,953] INFO Replica loaded for partition test-cluster-topic1-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,956] INFO Replica loaded for partition test-cluster-topic1-1 with initial high watermark 1 (kafka.cluster.Replica)
[2019-02-19 20:24:32,958] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 2 (kafka.cluster.Replica)
[2019-02-19 20:24:32,961] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,963] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:24:32,965] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions Set(test-cluster-topic-2, test-cluster-topic-0, test-cluster-topic1-2, test-cluster-topic-1, test-cluster-topic1-3, test-cluster-topic1-1, test-cluster-topic-3) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:32,993] INFO [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:32,996] INFO [ReplicaFetcherManager on broker 3] Added fetcher to broker BrokerEndPoint(id=1, host=DESKTOP-6IV0LP1:9091) for partitions Map(test-cluster-topic1-3 -> (offset=2, leaderEpoch=1), test-cluster-topic-2 -> (offset=0, leaderEpoch=2), test-cluster-topic-3 -> (offset=0, leaderEpoch=2), test-cluster-topic-1 -> (offset=0, leaderEpoch=1), test-cluster-topic1-2 -> (offset=2, leaderEpoch=1), test-cluster-topic-0 -> (offset=0, leaderEpoch=1)) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:33,001] INFO [ReplicaFetcherManager on broker 3] Added fetcher to broker BrokerEndPoint(id=2, host=DESKTOP-6IV0LP1:9092) for partitions Map(test-cluster-topic1-1 -> (offset=1, leaderEpoch=2)) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:33,001] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:33,013] WARN [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-3. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:33,018] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka3-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:33,018] INFO [Log partition=test-cluster-topic1-1, dir=D:\tmp\kafka3-logs] Truncating to 1 has no effect as the largest offset in the log is 0 (kafka.log.Log)
[2019-02-19 20:24:33,032] INFO [Log partition=test-cluster-topic1-3, dir=D:\tmp\kafka3-logs] Truncating to 2 has no effect as the largest offset in the log is 1 (kafka.log.Log)
[2019-02-19 20:24:33,034] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka3-logs] Truncating to 2 has no effect as the largest offset in the log is 1 (kafka.log.Log)
[2019-02-19 20:24:33,034] WARN [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-0. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:33,035] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka3-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:33,035] WARN [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-1. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:33,043] INFO [Log partition=test-cluster-topic-1, dir=D:\tmp\kafka3-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:33,043] WARN [ReplicaFetcher replicaId=3, leaderId=1, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-2. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:24:33,044] INFO [Log partition=test-cluster-topic-2, dir=D:\tmp\kafka3-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:24:33,050] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions Set(__consumer_offsets-8, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-23, __consumer_offsets-47, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-44, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-32) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:24:33,051] INFO [Partition test-cluster-topic1-3 broker=1] Expanding ISR from 1 to 1,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,060] INFO [Partition __consumer_offsets-29 broker=3] __consumer_offsets-29 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,090] INFO [Partition test-cluster-topic-2 broker=1] Expanding ISR from 1,2 to 1,2,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,114] INFO [Partition __consumer_offsets-26 broker=3] __consumer_offsets-26 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,124] INFO [Partition test-cluster-topic-3 broker=1] Expanding ISR from 1,2 to 1,2,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,157] INFO [Partition test-cluster-topic-1 broker=1] Expanding ISR from 1,2 to 1,2,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,166] INFO [Partition __consumer_offsets-23 broker=3] __consumer_offsets-23 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,182] INFO [Partition test-cluster-topic-0 broker=1] Expanding ISR from 1,2 to 1,2,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,209] INFO [Partition test-cluster-topic1-1 broker=2] Expanding ISR from 2 to 2,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,224] INFO [Partition __consumer_offsets-20 broker=3] __consumer_offsets-20 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,278] INFO [Partition test-cluster-topic1-2 broker=1] Expanding ISR from 1 to 1,3 (kafka.cluster.Partition)
[2019-02-19 20:24:33,282] INFO [Partition __consumer_offsets-17 broker=3] __consumer_offsets-17 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,326] INFO [Partition __consumer_offsets-14 broker=3] __consumer_offsets-14 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,376] INFO [Partition __consumer_offsets-11 broker=3] __consumer_offsets-11 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,401] INFO [Partition __consumer_offsets-8 broker=3] __consumer_offsets-8 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,426] INFO [Partition __consumer_offsets-5 broker=3] __consumer_offsets-5 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,450] INFO [Partition __consumer_offsets-2 broker=3] __consumer_offsets-2 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,476] INFO [Partition __consumer_offsets-47 broker=3] __consumer_offsets-47 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,517] INFO [Partition __consumer_offsets-38 broker=3] __consumer_offsets-38 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,543] INFO [Partition __consumer_offsets-35 broker=3] __consumer_offsets-35 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,577] INFO [Partition __consumer_offsets-44 broker=3] __consumer_offsets-44 starts at Leader Epoch 2 from offset 2. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,616] INFO [Partition __consumer_offsets-32 broker=3] __consumer_offsets-32 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,644] INFO [Partition __consumer_offsets-41 broker=3] __consumer_offsets-41 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:24:33,680] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,681] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,682] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,684] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,685] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,685] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,686] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,687] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,688] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,689] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,690] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,693] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,694] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,695] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,695] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,695] INFO [GroupMetadataManager brokerId=3] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,711] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-2 in 30 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,713] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,714] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,714] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,717] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,717] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,718] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,719] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-23 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,719] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,720] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,720] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,721] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,722] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,722] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,747] INFO [GroupCoordinator 3]: Loading group metadata for console-consumer-47624 with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:33,748] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-44 in 25 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:33,748] INFO [GroupMetadataManager brokerId=3] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:24:40,741] INFO [GroupCoordinator 2]: Member consumer-1-cc57d1b9-10f5-41ec-ab34-ce7f5abd1cd4 in group console-consumer-47013 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:40,745] INFO [GroupCoordinator 2]: Preparing to rebalance group console-consumer-47013 in state PreparingRebalance with old generation 1 (__consumer_offsets-46) (reason: removing member consumer-1-cc57d1b9-10f5-41ec-ab34-ce7f5abd1cd4 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:24:40,750] INFO [GroupCoordinator 2]: Group console-consumer-47013 with generation 2 is now empty (__consumer_offsets-46) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:13,452] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 2 (__consumer_offsets-12) (reason: Adding new member consumer-1-d0aff08d-3f3a-4ac8-84de-0b8951609a0b) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:13,458] INFO [GroupCoordinator 1]: Stabilized group test-group generation 3 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:13,469] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:18,793] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 3 (__consumer_offsets-12) (reason: Adding new member consumer-1-700b7027-23e2-4ec5-bb76-7974559ba4a0) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:19,517] INFO [GroupCoordinator 1]: Stabilized group test-group generation 4 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:19,521] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 4 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:22,190] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 4 (__consumer_offsets-12) (reason: Adding new member consumer-1-66d79446-1b6b-4bf9-a504-15b75b1eccfd) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:22,541] INFO [GroupCoordinator 1]: Stabilized group test-group generation 5 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:27:22,547] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:29:33,142] INFO [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(test-cluster-topic-0, test-cluster-topic-3) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:29:33,143] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions Set(test-cluster-topic-3, test-cluster-topic1-2, test-cluster-topic-0) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:29:33,148] INFO [Partition test-cluster-topic-3 broker=3] test-cluster-topic-3 starts at Leader Epoch 3 from offset 0. Previous Leader Epoch was: 2 (kafka.cluster.Partition)
[2019-02-19 20:29:33,146] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-cluster-topic-0, test-cluster-topic1-2, test-cluster-topic-3) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:29:33,156] INFO [ReplicaFetcherManager on broker 2] Added fetcher to broker BrokerEndPoint(id=3, host=DESKTOP-6IV0LP1:9093) for partitions Map(test-cluster-topic-3 -> (offset=0, leaderEpoch=3), test-cluster-topic-0 -> (offset=0, leaderEpoch=2)) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:29:33,173] INFO [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:29:33,175] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-3. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:29:33,175] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:29:33,176] WARN [ReplicaFetcher replicaId=2, leaderId=3, fetcherId=0] Based on replica's leader epoch, leader replied with an unknown offset in test-cluster-topic-0. The initial fetch offset 0 will be used for truncation. (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:29:33,176] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka2-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:29:33,196] INFO [ReplicaFetcherManager on broker 1] Added fetcher to broker BrokerEndPoint(id=3, host=DESKTOP-6IV0LP1:9093) for partitions Map(test-cluster-topic-3 -> (offset=0, leaderEpoch=3), test-cluster-topic1-2 -> (offset=3, leaderEpoch=2), test-cluster-topic-0 -> (offset=0, leaderEpoch=2)) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:29:33,197] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[2019-02-19 20:29:33,214] INFO [Partition test-cluster-topic1-2 broker=3] test-cluster-topic1-2 starts at Leader Epoch 2 from offset 3. Previous Leader Epoch was: 1 (kafka.cluster.Partition)
[2019-02-19 20:29:33,302] INFO [Partition test-cluster-topic-0 broker=3] test-cluster-topic-0 starts at Leader Epoch 2 from offset 0. Previous Leader Epoch was: 1 (kafka.cluster.Partition)
[2019-02-19 20:29:33,333] INFO [Log partition=test-cluster-topic-3, dir=D:\tmp\kafka1-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:29:33,333] INFO [Log partition=test-cluster-topic1-2, dir=D:\tmp\kafka1-logs] Truncating to 3 has no effect as the largest offset in the log is 2 (kafka.log.Log)
[2019-02-19 20:29:33,334] INFO [Log partition=test-cluster-topic-0, dir=D:\tmp\kafka1-logs] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[2019-02-19 20:34:27,550] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:34:29,680] INFO [GroupMetadataManager brokerId=2] Group console-consumer-47013 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:34:29,691] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 16 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:34:32,695] INFO [GroupMetadataManager brokerId=3] Group console-consumer-47624 transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:34:32,711] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 21 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:40:10,104] WARN Exception causing close of session 0x1000a8f6f890001: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:10,105] INFO Closed socket connection for client /127.0.0.1:58602 which had sessionid 0x1000a8f6f890001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:10,109] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=155772428, epoch=1867) to node 2: java.io.IOException: Connection to 2 was disconnected before the response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2019-02-19 20:40:10,111] WARN [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=3, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=155772428, epoch=1867)) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 2 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:97)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:97)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:190)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:241)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:129)
	at scala.Option.foreach(Option.scala:257)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:129)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:111)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-02-19 20:40:11,951] WARN Exception causing close of session 0x1000a8f6f890002: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:11,951] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:58622 which had sessionid 0x1000a8f6f890002 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:11,956] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error sending fetch request (sessionId=1305837098, epoch=1272) to node 3: java.io.IOException: Connection to 3 was disconnected before the response was read. (org.apache.kafka.clients.FetchSessionHandler)
[2019-02-19 20:40:11,957] WARN [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=1, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=1305837098, epoch=1272)) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 3 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:97)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:97)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:190)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:241)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:130)
	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:129)
	at scala.Option.foreach(Option.scala:257)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:129)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:111)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
[2019-02-19 20:40:13,119] WARN Exception causing close of session 0x1000a8f6f890000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:13,119] INFO Closed socket connection for client /127.0.0.1:58598 which had sessionid 0x1000a8f6f890000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:40:16,703] INFO Expiring session 0x1000a8f6f890001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:40:16,703] INFO Processed session termination for sessionid: 0x1000a8f6f890001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:44:49,558] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 20:44:49,560] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:44:49,560] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:44:49,560] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 20:44:49,561] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-02-19 20:44:49,573] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 20:44:49,573] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-02-19 20:44:54,106] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,106] INFO Server environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,108] INFO Server environment:java.version=1.8.0_201 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,110] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,111] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,111] INFO Server environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,113] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,113] INFO Server environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,114] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,114] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,114] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,115] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,115] INFO Server environment:user.name=Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,120] INFO Server environment:user.home=C:\Users\Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,120] INFO Server environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,135] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,135] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,136] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:44:54,163] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-02-19 20:44:54,167] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:45:21,004] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:45:21,499] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:45:21,500] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:45:21,518] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:24,258] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:45:24,746] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:45:24,747] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:45:24,765] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:26,055] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,055] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,056] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,056] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,056] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,056] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,057] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,057] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,057] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,057] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,057] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,058] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,058] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,058] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,058] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,060] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:26,076] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:26,076] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:26,078] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:26,078] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:59757 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:45:26,083] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:59757 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:26,085] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-02-19 20:45:26,105] INFO Established session 0x1000aa3c31f0000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:59757 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:26,107] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000aa3c31f0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:26,110] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:26,184] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:26,233] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:26,271] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:26,600] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:26,701] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 20:45:26,718] WARN No meta.properties file under dir D:\tmp\kafka1-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:26,793] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:26,803] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:26,828] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:26,828] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:26,829] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:26,855] INFO Log directory D:\tmp\kafka1-logs not found, creating it. (kafka.log.LogManager)
[2019-02-19 20:45:26,863] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:45:26,871] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2019-02-19 20:45:26,884] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:45:26,888] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:45:27,151] INFO Awaiting socket connections on 0.0.0.0:9091. (kafka.network.Acceptor)
[2019-02-19 20:45:27,182] INFO [SocketServer brokerId=1] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:45:27,204] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:27,205] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:27,205] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:27,221] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:45:27,536] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:45:28,034] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:45:28,036] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:45:28,054] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:29,293] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,293] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,293] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,294] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,294] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,294] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,295] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,296] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,296] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,296] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,297] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:29,314] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:29,315] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:29,316] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:59778 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:45:29,316] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:29,319] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:59778 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:29,333] INFO Established session 0x1000aa3c31f0001 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:59778 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:29,335] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000aa3c31f0001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:29,338] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:29,379] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x1 zxid:0x19 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,406] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x2 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,418] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x3 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,430] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x4 zxid:0x1c txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,442] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x5 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,456] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x6 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,467] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x7 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,480] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x8 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,492] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0x9 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,506] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0xa zxid:0x22 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,518] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0xb zxid:0x23 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,530] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0xc zxid:0x24 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,543] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0001 type:create cxid:0xd zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:29,718] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 20:45:29,721] WARN No meta.properties file under dir D:\tmp\kafka2-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:29,787] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:29,796] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:29,822] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:29,822] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:29,823] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:29,844] INFO Log directory D:\tmp\kafka2-logs not found, creating it. (kafka.log.LogManager)
[2019-02-19 20:45:29,851] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:45:29,859] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2019-02-19 20:45:29,871] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:45:29,875] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:45:30,124] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 20:45:30,153] INFO [SocketServer brokerId=2] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:45:30,174] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:30,174] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:30,175] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:30,187] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:45:31,789] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:31,803] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:31,804] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9091,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:31,807] WARN No meta.properties file under dir D:\tmp\kafka1-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:31,940] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:31,943] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:31,943] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:31,960] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:31,966] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:31,967] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:31,983] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 16 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:32,012] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:45:32,034] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:32,037] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:32,037] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:45:32,108] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:45:32,117] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:multi cxid:0x35 zxid:0x2a txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,138] INFO [SocketServer brokerId=1] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:45:32,144] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:multi cxid:0x38 zxid:0x2b txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,146] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:32,147] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:32,149] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[2019-02-19 20:45:32,208] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:setData cxid:0x40 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/config/topics/test-cluster-topic1 Error:KeeperErrorCode = NoNode for /config/topics/test-cluster-topic1 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,236] INFO Topic creation Map(test-cluster-topic1-0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[2019-02-19 20:45:32,255] INFO [KafkaApi-1] Auto creation of topic test-cluster-topic1 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-19 20:45:32,282] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0000 type:setData cxid:0x49 zxid:0x2f txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,319] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(1), __consumer_offsets-30 -> ArrayBuffer(1), __consumer_offsets-8 -> ArrayBuffer(1), __consumer_offsets-21 -> ArrayBuffer(1), __consumer_offsets-4 -> ArrayBuffer(1), __consumer_offsets-27 -> ArrayBuffer(1), __consumer_offsets-7 -> ArrayBuffer(1), __consumer_offsets-9 -> ArrayBuffer(1), __consumer_offsets-46 -> ArrayBuffer(1), __consumer_offsets-25 -> ArrayBuffer(1), __consumer_offsets-35 -> ArrayBuffer(1), __consumer_offsets-41 -> ArrayBuffer(1), __consumer_offsets-33 -> ArrayBuffer(1), __consumer_offsets-23 -> ArrayBuffer(1), __consumer_offsets-49 -> ArrayBuffer(1), __consumer_offsets-47 -> ArrayBuffer(1), __consumer_offsets-16 -> ArrayBuffer(1), __consumer_offsets-28 -> ArrayBuffer(1), __consumer_offsets-31 -> ArrayBuffer(1), __consumer_offsets-36 -> ArrayBuffer(1), __consumer_offsets-42 -> ArrayBuffer(1), __consumer_offsets-3 -> ArrayBuffer(1), __consumer_offsets-18 -> ArrayBuffer(1), __consumer_offsets-37 -> ArrayBuffer(1), __consumer_offsets-15 -> ArrayBuffer(1), __consumer_offsets-24 -> ArrayBuffer(1), __consumer_offsets-38 -> ArrayBuffer(1), __consumer_offsets-17 -> ArrayBuffer(1), __consumer_offsets-48 -> ArrayBuffer(1), __consumer_offsets-19 -> ArrayBuffer(1), __consumer_offsets-11 -> ArrayBuffer(1), __consumer_offsets-13 -> ArrayBuffer(1), __consumer_offsets-2 -> ArrayBuffer(1), __consumer_offsets-43 -> ArrayBuffer(1), __consumer_offsets-6 -> ArrayBuffer(1), __consumer_offsets-14 -> ArrayBuffer(1), __consumer_offsets-20 -> ArrayBuffer(1), __consumer_offsets-0 -> ArrayBuffer(1), __consumer_offsets-44 -> ArrayBuffer(1), __consumer_offsets-39 -> ArrayBuffer(1), __consumer_offsets-12 -> ArrayBuffer(1), __consumer_offsets-45 -> ArrayBuffer(1), __consumer_offsets-1 -> ArrayBuffer(1), __consumer_offsets-5 -> ArrayBuffer(1), __consumer_offsets-26 -> ArrayBuffer(1), __consumer_offsets-29 -> ArrayBuffer(1), __consumer_offsets-34 -> ArrayBuffer(1), __consumer_offsets-10 -> ArrayBuffer(1), __consumer_offsets-32 -> ArrayBuffer(1), __consumer_offsets-40 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[2019-02-19 20:45:32,344] INFO [KafkaApi-1] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-19 20:45:32,403] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(test-cluster-topic1-0) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:45:32,466] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:32,476] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[2019-02-19 20:45:32,477] INFO Created log for partition test-cluster-topic1-0 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:32,478] INFO [Partition test-cluster-topic1-0 broker=1] No checkpointed highwatermark is found for partition test-cluster-topic1-0 (kafka.cluster.Partition)
[2019-02-19 20:45:32,480] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:32,485] INFO [Partition test-cluster-topic1-0 broker=1] test-cluster-topic1-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:32,583] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,583] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,584] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,584] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,584] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,584] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,586] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,586] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,587] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,587] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,587] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,588] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,588] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,588] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,588] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,590] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:45:32,606] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:32,607] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:32,608] INFO Accepted socket connection from /127.0.0.1:59801 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:45:32,609] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:32,611] INFO Client attempting to establish new session at /127.0.0.1:59801 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:32,641] INFO Established session 0x1000aa3c31f0002 with negotiated timeout 6000 for client /127.0.0.1:59801 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:45:32,643] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000aa3c31f0002, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:45:32,649] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:45:32,698] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x1 zxid:0x7e txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,730] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x2 zxid:0x8d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,752] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x3 zxid:0x96 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,785] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x4 zxid:0x9e txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,819] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x5 zxid:0x9f txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,837] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x6 zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,862] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x7 zxid:0xa1 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,884] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x8 zxid:0xa2 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,893] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:45:32,899] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0x9 zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,902] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:32,903] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:32,904] INFO Created log for partition __consumer_offsets-0 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:32,905] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-02-19 20:45:32,905] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:32,905] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:32,917] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0xa zxid:0xa4 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,960] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0xb zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,984] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0xc zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:32,987] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:32,988] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:32,989] INFO Created log for partition __consumer_offsets-29 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:32,990] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-02-19 20:45:32,990] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:32,990] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:32,998] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:create cxid:0xd zxid:0xa7 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:45:33,042] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,051] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-02-19 20:45:33,052] INFO Created log for partition __consumer_offsets-48 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,054] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-02-19 20:45:33,055] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,055] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,087] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,088] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,089] INFO Created log for partition __consumer_offsets-10 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,089] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-02-19 20:45:33,089] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,090] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,120] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,121] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,122] INFO Created log for partition __consumer_offsets-45 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,122] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-02-19 20:45:33,122] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,122] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,155] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,156] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,157] INFO Created log for partition __consumer_offsets-26 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,158] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-02-19 20:45:33,158] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,158] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,196] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 20:45:33,198] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,199] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:33,200] INFO Created log for partition __consumer_offsets-7 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,201] WARN No meta.properties file under dir D:\tmp\kafka3-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:33,201] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-02-19 20:45:33,201] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,202] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,275] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:33,284] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:45:33,297] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,298] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,299] INFO Created log for partition __consumer_offsets-42 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,300] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-02-19 20:45:33,300] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,300] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,314] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:33,317] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:33,315] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:45:33,333] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,334] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,335] INFO Created log for partition __consumer_offsets-4 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,335] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-02-19 20:45:33,335] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,336] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,350] INFO Log directory D:\tmp\kafka3-logs not found, creating it. (kafka.log.LogManager)
[2019-02-19 20:45:33,357] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:45:33,366] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2019-02-19 20:45:33,374] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,377] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 20:45:33,378] INFO Created log for partition __consumer_offsets-23 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,381] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-02-19 20:45:33,381] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,382] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,392] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:45:33,396] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:45:33,421] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,422] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,423] INFO Created log for partition __consumer_offsets-1 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,423] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,423] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,424] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,455] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,456] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,458] INFO Created log for partition __consumer_offsets-20 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,462] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-02-19 20:45:33,462] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,462] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,496] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,497] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,498] INFO Created log for partition __consumer_offsets-39 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,499] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-02-19 20:45:33,499] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,499] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,530] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,531] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,532] INFO Created log for partition __consumer_offsets-17 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,532] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-02-19 20:45:33,533] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,533] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,563] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,564] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,565] INFO Created log for partition __consumer_offsets-36 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,566] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-02-19 20:45:33,566] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,566] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,605] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,606] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,607] INFO Created log for partition __consumer_offsets-14 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,608] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-02-19 20:45:33,608] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,608] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,638] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,639] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,640] INFO Created log for partition __consumer_offsets-33 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,641] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-02-19 20:45:33,641] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,641] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,672] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,674] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,675] INFO Created log for partition __consumer_offsets-49 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,680] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-02-19 20:45:33,680] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.Acceptor)
[2019-02-19 20:45:33,681] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,681] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,709] INFO [SocketServer brokerId=3] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:45:33,713] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,714] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,715] INFO Created log for partition __consumer_offsets-11 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,716] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-02-19 20:45:33,716] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,716] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,732] INFO [ExpirationReaper-3-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:33,734] INFO [ExpirationReaper-3-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:33,734] INFO [ExpirationReaper-3-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:33,746] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:45:33,766] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,767] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,768] INFO Created log for partition __consumer_offsets-30 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,768] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-02-19 20:45:33,768] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,769] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,805] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,806] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,807] INFO Created log for partition __consumer_offsets-46 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,807] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-02-19 20:45:33,807] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,807] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,838] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,839] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,840] INFO Created log for partition __consumer_offsets-27 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,841] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-02-19 20:45:33,841] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,841] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,880] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,882] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,883] INFO Created log for partition __consumer_offsets-8 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,884] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-02-19 20:45:33,884] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,884] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,920] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,921] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,922] INFO Created log for partition __consumer_offsets-24 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,922] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-02-19 20:45:33,922] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,923] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,956] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,957] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:33,958] INFO Created log for partition __consumer_offsets-43 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,958] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-02-19 20:45:33,959] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,959] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:33,989] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:33,990] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:33,990] INFO Created log for partition __consumer_offsets-5 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:33,991] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-02-19 20:45:33,991] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:33,991] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,034] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,036] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,037] INFO Created log for partition __consumer_offsets-21 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,038] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-02-19 20:45:34,038] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,038] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,076] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,077] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,078] INFO Created log for partition __consumer_offsets-2 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,078] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-02-19 20:45:34,079] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,080] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,117] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,119] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,121] INFO Created log for partition __consumer_offsets-40 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,122] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-02-19 20:45:34,122] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,122] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,161] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,170] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[2019-02-19 20:45:34,172] INFO Created log for partition __consumer_offsets-37 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,173] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-02-19 20:45:34,173] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,174] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,206] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,207] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:34,208] INFO Created log for partition __consumer_offsets-18 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,208] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-02-19 20:45:34,208] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,209] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,248] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,249] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:34,250] INFO Created log for partition __consumer_offsets-34 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,250] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-02-19 20:45:34,250] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,251] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,281] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,282] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:34,283] INFO Created log for partition __consumer_offsets-15 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,283] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-02-19 20:45:34,283] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,283] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,314] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,315] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:34,316] INFO Created log for partition __consumer_offsets-12 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,316] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-02-19 20:45:34,316] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,317] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,369] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,371] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 20:45:34,372] INFO Created log for partition __consumer_offsets-31 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,373] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-02-19 20:45:34,373] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,374] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,415] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,416] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:34,418] INFO Created log for partition __consumer_offsets-9 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,419] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-02-19 20:45:34,419] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,419] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,459] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,460] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:34,463] INFO Created log for partition __consumer_offsets-47 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,472] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-02-19 20:45:34,472] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,472] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,505] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,506] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:34,507] INFO Created log for partition __consumer_offsets-19 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,507] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-02-19 20:45:34,508] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,508] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,551] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,553] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,554] INFO Created log for partition __consumer_offsets-28 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,555] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-02-19 20:45:34,555] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,556] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,613] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,614] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,616] INFO Created log for partition __consumer_offsets-38 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,617] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-02-19 20:45:34,617] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,617] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,660] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,662] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2019-02-19 20:45:34,663] INFO Created log for partition __consumer_offsets-35 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,664] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-02-19 20:45:34,665] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,665] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,701] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,703] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,705] INFO Created log for partition __consumer_offsets-44 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,705] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-02-19 20:45:34,705] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,706] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,751] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,753] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,754] INFO Created log for partition __consumer_offsets-6 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,755] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-02-19 20:45:34,755] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,756] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,788] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:34,790] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,790] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:34,791] INFO Created log for partition __consumer_offsets-25 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,792] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-02-19 20:45:34,792] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,792] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,810] INFO Result of znode creation at /brokers/ids/2 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:34,812] INFO Registered broker 2 at path /brokers/ids/2 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:34,815] WARN No meta.properties file under dir D:\tmp\kafka2-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:34,881] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,882] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 20:45:34,884] INFO Created log for partition __consumer_offsets-16 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,885] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-02-19 20:45:34,885] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,885] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,915] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,916] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 20:45:34,918] INFO Created log for partition __consumer_offsets-22 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:34,919] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-02-19 20:45:34,919] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:34,919] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:34,953] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:34,953] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:34,955] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:34,986] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:34,990] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:34,992] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:34,995] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-19 20:45:34,998] INFO Created log for partition __consumer_offsets-41 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:35,005] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-02-19 20:45:35,005] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:35,006] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:35,010] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,028] INFO [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:45:35,049] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:35,052] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:35,053] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:45:35,104] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:45:35,124] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:35,143] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[2019-02-19 20:45:35,144] INFO Created log for partition __consumer_offsets-32 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:35,145] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-02-19 20:45:35,145] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:35,146] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:35,141] INFO [SocketServer brokerId=2] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:45:35,153] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:35,161] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:35,234] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
[2019-02-19 20:45:35,272] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:35,273] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:35,274] INFO Created log for partition __consumer_offsets-3 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:35,274] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-02-19 20:45:35,274] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:35,274] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:35,311] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:45:35,312] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 20:45:35,313] INFO Created log for partition __consumer_offsets-13 in D:\tmp\kafka1-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 20:45:35,313] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-02-19 20:45:35,314] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:45:35,314] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:45:35,359] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,360] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,361] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,361] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,361] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,362] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,362] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,362] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,363] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,363] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,364] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,364] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,364] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,364] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,365] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,365] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,365] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,366] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,366] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,366] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,367] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,367] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,367] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,368] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,368] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,368] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,368] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,369] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,369] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,369] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,369] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,370] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,370] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,370] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,370] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,372] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,372] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,373] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,374] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,374] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,382] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 21 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,383] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,384] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,384] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,384] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,385] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,385] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,385] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,386] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,387] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,387] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,388] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,390] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,391] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,391] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,391] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,392] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,392] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,392] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,392] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,393] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,393] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,393] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,394] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,394] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,396] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,397] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,398] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,399] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,401] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,401] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,402] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,402] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,403] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,403] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,404] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,405] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,405] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,406] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,406] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,439] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,460] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,464] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,466] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,468] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,468] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,468] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,469] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:35,484] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 0 (__consumer_offsets-12) (reason: Adding new member consumer-1-d0a27e56-7ea2-4e74-b8b1-f6cb241890aa) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:35,502] INFO [GroupCoordinator 1]: Stabilized group test-group generation 1 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:35,510] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:35,566] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 1 (__consumer_offsets-12) (reason: Adding new member consumer-1-fe9a192c-a3c3-4040-b4d6-2097dc84f922) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:38,351] INFO Creating /brokers/ids/3 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:38,367] INFO Result of znode creation at /brokers/ids/3 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:38,369] INFO Registered broker 3 at path /brokers/ids/3 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9093,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:45:38,373] WARN No meta.properties file under dir D:\tmp\kafka3-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 20:45:38,462] INFO [ExpirationReaper-3-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:38,465] INFO [ExpirationReaper-3-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:38,466] INFO [ExpirationReaper-3-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:45:38,487] INFO [GroupCoordinator 3]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:38,489] INFO [GroupCoordinator 3]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:38,494] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:45:38,522] INFO [ProducerId Manager 3]: Acquired new producerId block (brokerId:3,blockStartProducerId:2000,blockEndProducerId:2999) by writing to Zk with path version 3 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:45:38,542] INFO [TransactionCoordinator id=3] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:38,544] INFO [Transaction Marker Channel Manager 3]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:45:38,544] INFO [TransactionCoordinator id=3] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:45:38,571] INFO [GroupCoordinator 1]: Stabilized group test-group generation 2 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:38,580] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 2 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:45:38,601] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:45:38,623] INFO [SocketServer brokerId=3] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:45:38,628] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:38,628] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:45:38,630] INFO [KafkaServer id=3] started (kafka.server.KafkaServer)
[2019-02-19 20:53:32,431] WARN Exception causing close of session 0x1000aa3c31f0001: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:53:32,434] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:59778 which had sessionid 0x1000aa3c31f0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:53:37,703] INFO Expiring session 0x1000aa3c31f0001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:53:37,704] INFO Processed session termination for sessionid: 0x1000aa3c31f0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:54:54,565] WARN Exception causing close of session 0x1000aa3c31f0000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:54:54,567] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:59757 which had sessionid 0x1000aa3c31f0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:55:01,703] INFO Expiring session 0x1000aa3c31f0000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:55:01,704] INFO Processed session termination for sessionid: 0x1000aa3c31f0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:55:01,975] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:multi cxid:0xcf zxid:0xaf txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:55:02,007] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0002 type:multi cxid:0xd1 zxid:0xb0 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:55:38,489] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:58:38,020] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:60172 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:58:38,023] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:60172 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:58:38,042] INFO Established session 0x1000aa3c31f0003 with negotiated timeout 30000 for client /0:0:0:0:0:0:0:1:60172 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:58:38,276] INFO Processed session termination for sessionid: 0x1000aa3c31f0003 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:38,296] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:60172 which had sessionid 0x1000aa3c31f0003 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 20:58:50,024] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:58:50,546] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:58:50,547] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:58:50,567] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:58:55,096] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,097] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,097] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,097] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,098] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,098] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,099] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,100] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,101] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,101] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,102] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,102] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,103] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,103] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,104] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,107] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:58:55,146] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:58:55,146] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:58:55,148] INFO Accepted socket connection from /127.0.0.1:60193 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:58:55,149] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:58:55,152] INFO Client attempting to establish new session at /127.0.0.1:60193 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:58:55,168] INFO Established session 0x1000aa3c31f0004 with negotiated timeout 6000 for client /127.0.0.1:60193 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:58:55,170] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000aa3c31f0004, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:58:55,174] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:58:55,214] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x1 zxid:0xb4 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,253] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x2 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,267] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x3 zxid:0xb6 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,285] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x4 zxid:0xb7 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,301] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x5 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,319] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x6 zxid:0xb9 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,333] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x7 zxid:0xba txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,353] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x8 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,367] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0x9 zxid:0xbc txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,386] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0xa zxid:0xbd txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,445] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0xb zxid:0xbe txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,510] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0xc zxid:0xbf txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,524] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0004 type:create cxid:0xd zxid:0xc0 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:58:55,719] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 20:58:55,797] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:58:55,807] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:58:55,838] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:58:55,840] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:58:55,838] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:58:55,880] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:58:55,889] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-02-19 20:58:55,907] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:58:55,911] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:58:56,209] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 20:58:56,240] INFO [SocketServer brokerId=2] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:58:56,262] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:58:56,263] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:58:56,263] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:58:56,277] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:58:56,533] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 20:58:57,043] INFO starting (kafka.server.KafkaServer)
[2019-02-19 20:58:57,054] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 20:58:57,089] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:59:00,865] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:00,981] INFO Result of znode creation at /brokers/ids/2 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:00,983] INFO Registered broker 2 at path /brokers/ids/2 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:01,117] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:01,119] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:01,118] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:01,139] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:59:01,141] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:59:01,146] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:01,173] INFO [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:3000,blockEndProducerId:3999) by writing to Zk with path version 4 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:59:01,193] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:59:01,195] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:59:01,199] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:59:01,234] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:59:01,256] INFO [SocketServer brokerId=2] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:59:01,261] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:59:01,263] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:59:01,264] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
[2019-02-19 20:59:01,614] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,614] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,615] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,615] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,615] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,615] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,616] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,616] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,616] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,617] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,617] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,617] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,617] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,618] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,618] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,619] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 20:59:01,636] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:59:01,636] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:59:01,638] INFO Accepted socket connection from /127.0.0.1:60219 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:59:01,639] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:59:01,642] INFO Client attempting to establish new session at /127.0.0.1:60219 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:59:01,669] INFO Established session 0x1000aa3c31f0005 with negotiated timeout 6000 for client /127.0.0.1:60219 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:59:01,671] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000aa3c31f0005, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 20:59:01,675] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 20:59:01,717] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x1 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,745] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x2 zxid:0xc5 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,763] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x3 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,777] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x4 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,797] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x5 zxid:0xc8 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,811] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x6 zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,830] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x7 zxid:0xca txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,845] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x8 zxid:0xcb txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,864] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0x9 zxid:0xcc txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,878] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0xa zxid:0xcd txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,897] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0xb zxid:0xce txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,911] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0xc zxid:0xcf txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:01,931] INFO Got user-level KeeperException when processing sessionid:0x1000aa3c31f0005 type:create cxid:0xd zxid:0xd0 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:02,108] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 20:59:02,183] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:59:02,193] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 20:59:02,221] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:59:02,221] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:59:02,222] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 20:59:02,260] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 20:59:02,331] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,333] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,392] INFO [ProducerStateManager partition=test-cluster-topic1-0] Writing producer snapshot at offset 20 (kafka.log.ProducerStateManager)
[2019-02-19 20:59:02,438] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 20 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,442] INFO [ProducerStateManager partition=test-cluster-topic1-0] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\test-cluster-topic1-0\00000000000000000020.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:59:02,452] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 20 in 158 ms (kafka.log.Log)
[2019-02-19 20:59:02,472] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,473] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,507] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,508] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[2019-02-19 20:59:02,523] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,524] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,555] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,556] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-02-19 20:59:02,571] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,572] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,606] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,607] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:02,621] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,622] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,663] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,665] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:59:02,678] WARN [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Found a corrupted index file corresponding to log file D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.log due to Corrupt time index found, time index file (D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1550638053591}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-02-19 20:59:02,678] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,700] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 115 (kafka.log.ProducerStateManager)
[2019-02-19 20:59:02,702] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,703] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,726] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 115 (kafka.log.ProducerStateManager)
[2019-02-19 20:59:02,766] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 115 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,768] INFO [ProducerStateManager partition=__consumer_offsets-12] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000115.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 20:59:02,769] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 115 in 101 ms (kafka.log.Log)
[2019-02-19 20:59:02,794] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,794] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,831] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,833] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 57 ms (kafka.log.Log)
[2019-02-19 20:59:02,849] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,850] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,882] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,884] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:02,901] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,902] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,931] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,932] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-02-19 20:59:02,946] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,946] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,973] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:02,974] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-02-19 20:59:02,988] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:02,988] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,024] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,025] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:03,040] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,040] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,073] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,074] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-02-19 20:59:03,088] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,088] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,114] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,115] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 38 ms (kafka.log.Log)
[2019-02-19 20:59:03,130] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,130] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,164] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,165] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:03,181] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,181] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,215] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,216] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:03,230] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,230] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,265] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,266] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:03,281] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,281] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,328] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,328] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 58 ms (kafka.log.Log)
[2019-02-19 20:59:03,342] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,343] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,373] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,375] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 42 ms (kafka.log.Log)
[2019-02-19 20:59:03,387] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,388] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,423] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,424] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:03,438] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,439] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,481] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,484] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-02-19 20:59:03,509] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,509] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,540] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,541] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 20:59:03,555] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,556] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,589] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,590] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:03,609] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,610] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,648] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,649] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-02-19 20:59:03,665] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,666] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,708] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,711] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-02-19 20:59:03,736] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,737] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,790] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,792] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 77 ms (kafka.log.Log)
[2019-02-19 20:59:03,815] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,815] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,857] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,858] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 61 ms (kafka.log.Log)
[2019-02-19 20:59:03,872] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,872] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,906] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,907] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:03,920] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,921] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,948] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,949] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-02-19 20:59:03,961] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:03,962] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,990] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:03,991] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 20:59:04,005] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,006] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,039] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,040] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:04,053] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,053] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,082] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,082] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 20:59:04,094] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,095] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,123] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,124] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 20:59:04,139] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,140] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,180] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,181] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 20:59:04,193] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,194] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,229] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,230] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:04,246] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,246] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,282] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,283] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 20:59:04,295] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,295] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,331] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,332] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:04,345] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,345] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,373] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,374] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 20:59:04,387] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,387] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,415] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,416] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 20:59:04,430] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,431] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,465] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,466] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 20:59:04,487] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,487] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,523] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,524] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-02-19 20:59:04,538] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,538] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,573] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,575] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 20:59:04,589] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,589] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,624] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,625] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 20:59:04,640] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,640] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,673] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,674] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:04,688] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,689] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,725] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,726] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-02-19 20:59:04,740] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,740] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,774] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,775] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 20:59:04,789] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,789] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,831] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,832] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 55 ms (kafka.log.Log)
[2019-02-19 20:59:04,845] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,846] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,882] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,883] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 20:59:04,895] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,896] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,973] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:04,975] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 90 ms (kafka.log.Log)
[2019-02-19 20:59:04,994] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:04,994] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,057] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,058] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 81 ms (kafka.log.Log)
[2019-02-19 20:59:05,070] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:05,071] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,098] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,099] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-02-19 20:59:05,111] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 20:59:05,111] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,163] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 20:59:05,165] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-02-19 20:59:05,170] INFO Logs loading complete in 2910 ms. (kafka.log.LogManager)
[2019-02-19 20:59:05,190] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 20:59:05,193] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 20:59:05,416] INFO Awaiting socket connections on 0.0.0.0:9091. (kafka.network.Acceptor)
[2019-02-19 20:59:05,449] INFO [SocketServer brokerId=1] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 20:59:05,470] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:05,471] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:05,471] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:05,483] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 20:59:10,085] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:10,107] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:10,109] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9091,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 20:59:10,353] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:10,354] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:10,354] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 20:59:10,398] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:59:10,399] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:59:10,404] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:10,465] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:4000,blockEndProducerId:4999) by writing to Zk with path version 5 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 20:59:10,528] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:59:10,537] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 20:59:10,538] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 20:59:10,575] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 20:59:10,591] INFO [SocketServer brokerId=1] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 20:59:10,596] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:59:10,596] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 20:59:10,598] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[2019-02-19 20:59:10,712] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, test-cluster-topic1-0, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:59:10,723] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,727] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,773] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,774] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,803] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,804] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,834] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,834] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,864] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,864] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,886] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,887] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,917] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,918] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,945] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,945] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:10,975] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:10,977] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,003] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,004] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,027] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,028] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,053] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,053] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,084] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,084] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,111] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,112] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,154] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,154] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,178] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,178] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,209] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,209] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,244] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,245] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,274] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,276] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,311] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,311] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,373] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,374] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,395] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,395] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,426] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,426] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,454] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,458] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,486] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,487] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,511] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,512] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,536] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,537] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,561] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,562] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,592] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,593] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,620] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,621] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,660] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,661] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,686] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,688] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,717] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,718] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,773] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 20 (kafka.cluster.Replica)
[2019-02-19 20:59:11,773] INFO [Partition test-cluster-topic1-0 broker=1] test-cluster-topic1-0 starts at Leader Epoch 0 from offset 20. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,779] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 114 (kafka.cluster.Replica)
[2019-02-19 20:59:11,784] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 0 from offset 115. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,797] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,797] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,828] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,828] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,868] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,869] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,895] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,896] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,920] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,925] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,945] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,946] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,970] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,970] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:11,995] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:11,995] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,020] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,020] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,044] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,045] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,073] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,073] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,097] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,098] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,133] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,135] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,162] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,162] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,187] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,187] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,214] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 20:59:12,215] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 20:59:12,271] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,273] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,274] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,276] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,277] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,277] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,278] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,279] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,280] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,281] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,282] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,283] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,286] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,286] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,287] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,287] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,288] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,288] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,288] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,290] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,290] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,289] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,291] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,292] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,292] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,292] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,293] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,297] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,298] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,299] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,299] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,300] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,300] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,301] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,301] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,301] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,301] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,302] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,303] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,303] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,304] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,304] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,304] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,307] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,308] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,308] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,309] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,309] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,310] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,310] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,310] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,310] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,311] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,311] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,311] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,311] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,312] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,312] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,312] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,313] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,312] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,313] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,313] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,313] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,314] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,314] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,314] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,314] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,314] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,315] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,315] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,315] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,316] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,316] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,318] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,319] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,319] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,319] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,320] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,320] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,321] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,321] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,321] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,322] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,324] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,332] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,333] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, test-cluster-topic1-0, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 20:59:12,334] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,336] WARN [LeaderEpochCache __consumer_offsets-0] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,363] INFO [GroupCoordinator 1]: Loading group metadata for test-group with generation 2 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 20:59:12,367] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 33 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,368] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,368] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,368] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,369] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,369] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,369] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,370] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,370] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,370] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,370] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,371] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,372] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 20:59:12,375] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,375] WARN [LeaderEpochCache __consumer_offsets-29] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,440] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,440] WARN [LeaderEpochCache __consumer_offsets-48] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,459] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,459] WARN [LeaderEpochCache __consumer_offsets-10] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,485] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,485] WARN [LeaderEpochCache __consumer_offsets-45] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,512] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,512] WARN [LeaderEpochCache __consumer_offsets-26] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,542] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,542] WARN [LeaderEpochCache __consumer_offsets-7] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,570] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,570] WARN [LeaderEpochCache __consumer_offsets-42] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,595] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,596] WARN [LeaderEpochCache __consumer_offsets-4] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,626] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,626] WARN [LeaderEpochCache __consumer_offsets-23] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,653] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,654] WARN [LeaderEpochCache __consumer_offsets-1] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,683] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,683] WARN [LeaderEpochCache __consumer_offsets-20] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,704] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,704] WARN [LeaderEpochCache __consumer_offsets-39] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,736] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,736] WARN [LeaderEpochCache __consumer_offsets-17] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,761] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,761] WARN [LeaderEpochCache __consumer_offsets-36] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,797] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,797] WARN [LeaderEpochCache __consumer_offsets-14] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,845] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,846] WARN [LeaderEpochCache __consumer_offsets-33] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,875] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,876] WARN [LeaderEpochCache __consumer_offsets-49] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,904] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,904] WARN [LeaderEpochCache __consumer_offsets-11] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,933] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,934] WARN [LeaderEpochCache __consumer_offsets-30] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,969] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,969] WARN [LeaderEpochCache __consumer_offsets-46] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:12,994] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:12,995] WARN [LeaderEpochCache __consumer_offsets-27] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,020] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,020] WARN [LeaderEpochCache __consumer_offsets-8] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,050] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,050] WARN [LeaderEpochCache __consumer_offsets-24] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,094] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,095] WARN [LeaderEpochCache __consumer_offsets-43] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,119] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,120] WARN [LeaderEpochCache __consumer_offsets-5] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,144] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,144] WARN [LeaderEpochCache __consumer_offsets-21] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,169] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,170] WARN [LeaderEpochCache __consumer_offsets-2] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,194] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,195] WARN [LeaderEpochCache __consumer_offsets-40] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,219] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,220] WARN [LeaderEpochCache __consumer_offsets-37] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,245] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,245] WARN [LeaderEpochCache __consumer_offsets-18] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,275] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,275] WARN [LeaderEpochCache __consumer_offsets-34] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,294] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,294] WARN [LeaderEpochCache __consumer_offsets-15] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,319] INFO [Partition test-cluster-topic1-0 broker=1] test-cluster-topic1-0 starts at Leader Epoch 1 from offset 32. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,349] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 1 from offset 116. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,376] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,376] WARN [LeaderEpochCache __consumer_offsets-31] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,403] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,403] WARN [LeaderEpochCache __consumer_offsets-9] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,434] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,434] WARN [LeaderEpochCache __consumer_offsets-47] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,453] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,454] WARN [LeaderEpochCache __consumer_offsets-19] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,478] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,479] WARN [LeaderEpochCache __consumer_offsets-28] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,503] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,503] WARN [LeaderEpochCache __consumer_offsets-38] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,528] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,528] WARN [LeaderEpochCache __consumer_offsets-35] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,553] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,553] WARN [LeaderEpochCache __consumer_offsets-44] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,578] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,578] WARN [LeaderEpochCache __consumer_offsets-6] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,603] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,603] WARN [LeaderEpochCache __consumer_offsets-25] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,628] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,628] WARN [LeaderEpochCache __consumer_offsets-16] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,653] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,653] WARN [LeaderEpochCache __consumer_offsets-22] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,678] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,679] WARN [LeaderEpochCache __consumer_offsets-41] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,703] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,704] WARN [LeaderEpochCache __consumer_offsets-32] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,728] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,729] WARN [LeaderEpochCache __consumer_offsets-3] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:13,753] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2019-02-19 20:59:13,753] WARN [LeaderEpochCache __consumer_offsets-13] New epoch entry EpochEntry(epoch=1, startOffset=0) caused truncation of conflicting entries ListBuffer(EpochEntry(epoch=0, startOffset=0)). Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2019-02-19 20:59:24,394] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:60290 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 20:59:24,396] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:60290 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:59:24,421] INFO Established session 0x1000aa3c31f0006 with negotiated timeout 30000 for client /0:0:0:0:0:0:0:1:60290 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 20:59:24,656] INFO Processed session termination for sessionid: 0x1000aa3c31f0006 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 20:59:24,670] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:60290 which had sessionid 0x1000aa3c31f0006 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:01:58,293] INFO Accepted socket connection from /127.0.0.1:60305 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:01:58,297] INFO Client attempting to establish new session at /127.0.0.1:60305 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:01:58,316] INFO Established session 0x1000aa3c31f0007 with negotiated timeout 30000 for client /127.0.0.1:60305 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:01:58,530] INFO Processed session termination for sessionid: 0x1000aa3c31f0007 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:01:58,550] INFO Closed socket connection for client /127.0.0.1:60305 which had sessionid 0x1000aa3c31f0007 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:02:36,985] WARN Exception causing close of session 0x1000aa3c31f0004: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:02:36,986] INFO Closed socket connection for client /127.0.0.1:60193 which had sessionid 0x1000aa3c31f0004 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:02:43,703] INFO Expiring session 0x1000aa3c31f0004, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:02:43,703] INFO Processed session termination for sessionid: 0x1000aa3c31f0004 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:02:49,006] INFO Accepted socket connection from /127.0.0.1:60311 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:02:49,009] INFO Client attempting to establish new session at /127.0.0.1:60311 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:02:49,036] INFO Established session 0x1000aa3c31f0008 with negotiated timeout 30000 for client /127.0.0.1:60311 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:02:49,271] INFO Processed session termination for sessionid: 0x1000aa3c31f0008 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:02:49,284] INFO Closed socket connection for client /127.0.0.1:60311 which had sessionid 0x1000aa3c31f0008 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:03:35,080] INFO Accepted socket connection from /127.0.0.1:60316 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:03:35,086] INFO Client attempting to establish new session at /127.0.0.1:60316 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:03:35,104] INFO Established session 0x1000aa3c31f0009 with negotiated timeout 30000 for client /127.0.0.1:60316 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:03:35,290] INFO Processed session termination for sessionid: 0x1000aa3c31f0009 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:03:35,312] INFO Closed socket connection for client /127.0.0.1:60316 which had sessionid 0x1000aa3c31f0009 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:05:38,489] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:06:25,817] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 2 (__consumer_offsets-12) (reason: removing member consumer-1-d0a27e56-7ea2-4e74-b8b1-f6cb241890aa on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:06:27,416] INFO [GroupCoordinator 1]: Stabilized group test-group generation 3 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:06:27,429] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:11,142] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 3 (__consumer_offsets-12) (reason: removing member consumer-1-dfb84918-e96c-4ca0-8615-f99026275236 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:12,446] INFO [GroupCoordinator 1]: Stabilized group test-group generation 4 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:12,449] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 4 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:21,049] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 4 (__consumer_offsets-12) (reason: Adding new member consumer-1-b89109c0-c2de-4923-a8b4-3c6553942e97) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:21,457] INFO [GroupCoordinator 1]: Stabilized group test-group generation 5 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:07:21,460] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:09:10,404] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:09:22,723] WARN Exception causing close of session 0x1000aa3c31f0002: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:09:22,723] INFO Closed socket connection for client /127.0.0.1:59801 which had sessionid 0x1000aa3c31f0002 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:09:26,666] WARN Exception causing close of session 0x1000aa3c31f0005: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:09:26,666] INFO Closed socket connection for client /127.0.0.1:60219 which had sessionid 0x1000aa3c31f0005 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:09:28,703] INFO Expiring session 0x1000aa3c31f0002, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:09:28,703] INFO Processed session termination for sessionid: 0x1000aa3c31f0002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:09:34,703] INFO Expiring session 0x1000aa3c31f0005, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:09:34,703] INFO Processed session termination for sessionid: 0x1000aa3c31f0005 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:30,715] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 21:10:30,718] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 21:10:30,718] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 21:10:30,720] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 21:10:30,720] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-02-19 21:10:30,734] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 21:10:30,734] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-02-19 21:10:34,982] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 21:10:35,266] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,266] INFO Server environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,268] INFO Server environment:java.version=1.8.0_201 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,271] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,271] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,271] INFO Server environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,281] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,289] INFO Server environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,290] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,290] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,290] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,290] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,291] INFO Server environment:user.name=Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,291] INFO Server environment:user.home=C:\Users\Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,291] INFO Server environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,299] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,299] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,300] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:35,319] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-02-19 21:10:35,321] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:10:35,520] INFO starting (kafka.server.KafkaServer)
[2019-02-19 21:10:35,521] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 21:10:35,540] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:39,196] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 21:10:39,690] INFO starting (kafka.server.KafkaServer)
[2019-02-19 21:10:39,691] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 21:10:39,710] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:40,068] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,068] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,068] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,068] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,068] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,068] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,069] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,069] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,070] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,070] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,070] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,070] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,071] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,071] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,071] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,073] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:40,091] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:40,092] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:40,094] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:40,094] INFO Accepted socket connection from /127.0.0.1:60820 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:10:40,100] INFO Client attempting to establish new session at /127.0.0.1:60820 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:40,101] INFO Creating new log file: log.111 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-02-19 21:10:40,133] INFO Established session 0x1000abb47650000 with negotiated timeout 6000 for client /127.0.0.1:60820 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:40,134] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000abb47650000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:40,138] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:40,188] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x1 zxid:0x112 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,216] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x2 zxid:0x113 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,229] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x3 zxid:0x114 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,248] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x4 zxid:0x115 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,264] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x5 zxid:0x116 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,281] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x6 zxid:0x117 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,298] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x7 zxid:0x118 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,339] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x8 zxid:0x119 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,356] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0x9 zxid:0x11a txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,373] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0xa zxid:0x11b txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,389] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0xb zxid:0x11c txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,406] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0xc zxid:0x11d txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,423] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:create cxid:0xd zxid:0x11e txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:40,596] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 21:10:40,676] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:40,695] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 1
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka1-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9091
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:40,725] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:40,728] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:40,728] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:40,772] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 21:10:40,849] WARN [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Found a corrupted index file corresponding to log file D:\tmp\kafka1-logs\test-cluster-topic1-0\00000000000000000000.log due to Corrupt time index found, time index file (D:\tmp\kafka1-logs\test-cluster-topic1-0\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1550638356049}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-02-19 21:10:40,853] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:40,889] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 21:10:40,909] INFO [ProducerStateManager partition=test-cluster-topic1-0] Writing producer snapshot at offset 42 (kafka.log.ProducerStateManager)
[2019-02-19 21:10:40,918] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:40,919] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:40,936] INFO [ProducerStateManager partition=test-cluster-topic1-0] Writing producer snapshot at offset 42 (kafka.log.ProducerStateManager)
[2019-02-19 21:10:40,976] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 42 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:40,977] INFO [ProducerStateManager partition=test-cluster-topic1-0] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\test-cluster-topic1-0\00000000000000000042.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 21:10:40,989] INFO [Log partition=test-cluster-topic1-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 42 in 178 ms (kafka.log.Log)
[2019-02-19 21:10:41,012] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,012] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,050] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,051] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 53 ms (kafka.log.Log)
[2019-02-19 21:10:41,066] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,067] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,098] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,099] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-02-19 21:10:41,119] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,119] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,149] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,151] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:41,165] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,165] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,208] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,209] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 21:10:41,222] WARN [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Found a corrupted index file corresponding to log file D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.log due to Corrupt time index found, time index file (D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000000.timeindex) has non-zero size but the last timestamp is 0 which is less than the first timestamp 1550638053591}, recovering segment and rebuilding index files... (kafka.log.Log)
[2019-02-19 21:10:41,222] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,247] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 242 (kafka.log.ProducerStateManager)
[2019-02-19 21:10:41,248] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,249] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,276] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 242 (kafka.log.ProducerStateManager)
[2019-02-19 21:10:41,300] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Loading producer state till offset 242 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,301] INFO [ProducerStateManager partition=__consumer_offsets-12] Loading producer state from snapshot file 'D:\tmp\kafka1-logs\__consumer_offsets-12\00000000000000000242.snapshot' (kafka.log.ProducerStateManager)
[2019-02-19 21:10:41,302] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 242 in 90 ms (kafka.log.Log)
[2019-02-19 21:10:41,316] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,317] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,350] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,352] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:41,366] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,366] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,405] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,407] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 21:10:41,428] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,428] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,444] INFO starting (kafka.server.KafkaServer)
[2019-02-19 21:10:41,445] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 21:10:41,458] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,459] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 21:10:41,464] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:41,474] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,474] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,507] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,508] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:41,522] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,522] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,558] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,558] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:41,572] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,573] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,599] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,600] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 38 ms (kafka.log.Log)
[2019-02-19 21:10:41,615] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,616] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,650] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,651] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:41,666] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,666] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,705] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,706] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[2019-02-19 21:10:41,720] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,720] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,749] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,750] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 41 ms (kafka.log.Log)
[2019-02-19 21:10:41,764] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,764] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,809] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,810] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 56 ms (kafka.log.Log)
[2019-02-19 21:10:41,823] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,823] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,858] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,859] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:41,874] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,874] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,909] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,910] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 21:10:41,922] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:41,923] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,973] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:41,976] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-02-19 21:10:42,002] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,007] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,041] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,042] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 60 ms (kafka.log.Log)
[2019-02-19 21:10:42,056] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,056] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,083] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,084] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 21:10:42,098] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,099] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,132] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,133] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:42,151] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,151] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,193] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,194] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 57 ms (kafka.log.Log)
[2019-02-19 21:10:42,211] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,211] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,250] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,251] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2019-02-19 21:10:42,264] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,264] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,300] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,301] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 21:10:42,316] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,317] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,350] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,351] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:42,366] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,366] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,399] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,400] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[2019-02-19 21:10:42,414] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,414] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,442] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,443] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[2019-02-19 21:10:42,460] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,460] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,497] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,498] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 21:10:42,511] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,512] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,542] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,543] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)
[2019-02-19 21:10:42,555] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,556] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,583] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,584] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-02-19 21:10:42,597] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,598] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,633] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,634] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 21:10:42,648] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,648] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,684] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,685] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 21:10:42,699] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,699] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,734] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,735] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:42,748] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,749] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,783] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,784] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:42,798] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,799] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,960] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:42,962] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 176 ms (kafka.log.Log)
[2019-02-19 21:10:42,983] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:42,984] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,017] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,018] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[2019-02-19 21:10:43,034] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,035] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,067] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,068] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:43,084] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,084] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,117] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,118] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:43,132] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,132] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,167] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,168] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:43,181] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,182] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,208] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,209] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[2019-02-19 21:10:43,222] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,222] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,259] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,261] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[2019-02-19 21:10:43,274] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,274] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,309] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,311] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 21:10:43,324] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,324] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,375] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,377] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 64 ms (kafka.log.Log)
[2019-02-19 21:10:43,390] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,391] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,426] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,428] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[2019-02-19 21:10:43,441] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,441] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,501] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,503] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 73 ms (kafka.log.Log)
[2019-02-19 21:10:43,527] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,527] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,568] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,569] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 63 ms (kafka.log.Log)
[2019-02-19 21:10:43,582] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,583] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,617] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,618] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[2019-02-19 21:10:43,631] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,631] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,677] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,679] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 59 ms (kafka.log.Log)
[2019-02-19 21:10:43,702] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,703] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,734] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,735] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 52 ms (kafka.log.Log)
[2019-02-19 21:10:43,751] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2019-02-19 21:10:43,751] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,784] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 21:10:43,785] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka1-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 48 ms (kafka.log.Log)
[2019-02-19 21:10:43,788] INFO Logs loading complete in 3016 ms. (kafka.log.LogManager)
[2019-02-19 21:10:43,805] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 21:10:43,808] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 21:10:44,068] INFO Awaiting socket connections on 0.0.0.0:9091. (kafka.network.Acceptor)
[2019-02-19 21:10:44,101] INFO [SocketServer brokerId=1] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 21:10:44,124] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:44,125] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:44,125] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:44,135] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 21:10:44,238] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,238] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,239] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,239] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,239] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,239] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,241] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,241] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,242] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,242] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,243] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,243] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,244] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,246] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,252] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,257] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:44,284] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:44,287] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:44,288] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:60841 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:10:44,288] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:44,291] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:60841 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:44,315] INFO Established session 0x1000abb47650001 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:60841 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:44,317] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000abb47650001, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:44,321] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:44,373] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x1 zxid:0x120 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,401] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x2 zxid:0x121 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,441] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x3 zxid:0x122 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,458] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x4 zxid:0x123 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,485] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x5 zxid:0x124 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,507] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x6 zxid:0x125 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,524] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x7 zxid:0x126 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,541] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x8 zxid:0x127 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,558] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0x9 zxid:0x128 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,616] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0xa zxid:0x129 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,658] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0xb zxid:0x12a txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,674] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0xc zxid:0x12b txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,691] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650001 type:create cxid:0xd zxid:0x12c txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:44,923] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 21:10:45,011] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:45,029] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:45,073] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:45,073] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:45,073] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:45,120] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 21:10:45,129] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-02-19 21:10:45,150] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 21:10:45,160] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 21:10:45,444] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 21:10:45,479] INFO [SocketServer brokerId=2] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 21:10:45,508] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:45,509] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:45,509] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:45,525] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 21:10:45,999] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,003] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,004] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,004] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,004] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,004] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,005] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,006] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,010] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,010] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,010] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,011] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,011] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,011] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,011] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,015] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:10:46,037] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:46,045] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:46,047] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:46,047] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:60856 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:10:46,071] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:60856 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:46,101] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000abb47650002, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:10:46,099] INFO Established session 0x1000abb47650002 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:60856 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:10:46,107] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:10:46,148] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x1 zxid:0x12e txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,175] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x2 zxid:0x12f txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,190] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x3 zxid:0x130 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,208] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x4 zxid:0x131 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,226] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x5 zxid:0x132 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,243] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x6 zxid:0x133 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,258] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x7 zxid:0x134 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,276] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x8 zxid:0x135 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,291] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0x9 zxid:0x136 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,309] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0xa zxid:0x137 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,325] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0xb zxid:0x138 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,342] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0xc zxid:0x139 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,358] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650002 type:create cxid:0xd zxid:0x13a txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:46,529] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 21:10:46,603] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:46,613] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 3
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka3-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9093
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:10:46,639] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:46,640] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:46,645] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:10:46,680] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 21:10:46,690] INFO Logs loading complete in 10 ms. (kafka.log.LogManager)
[2019-02-19 21:10:46,706] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 21:10:46,710] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 21:10:46,966] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.Acceptor)
[2019-02-19 21:10:46,999] INFO [SocketServer brokerId=3] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 21:10:47,020] INFO [ExpirationReaper-3-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:47,022] INFO [ExpirationReaper-3-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:47,022] INFO [ExpirationReaper-3-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:47,035] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 21:10:48,728] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:48,751] INFO Result of znode creation at /brokers/ids/1 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:48,752] INFO Registered broker 1 at path /brokers/ids/1 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9091,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:48,809] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:48,809] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:48,809] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:48,833] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:48,835] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:48,838] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:48,878] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:5000,blockEndProducerId:5999) by writing to Zk with path version 6 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 21:10:48,916] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:48,939] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 21:10:48,939] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:48,982] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 21:10:48,995] INFO [SocketServer brokerId=1] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 21:10:49,000] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:49,003] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:49,006] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[2019-02-19 21:10:49,080] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:multi cxid:0x65 zxid:0x13e txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:49,107] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650000 type:multi cxid:0x67 zxid:0x13f txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:10:49,110] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, test-cluster-topic1-0, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 21:10:49,135] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,142] INFO [Partition __consumer_offsets-0 broker=1] __consumer_offsets-0 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,183] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,184] INFO [Partition __consumer_offsets-29 broker=1] __consumer_offsets-29 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,205] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,205] INFO [Partition __consumer_offsets-48 broker=1] __consumer_offsets-48 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,236] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,236] INFO [Partition __consumer_offsets-10 broker=1] __consumer_offsets-10 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,268] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,268] INFO [Partition __consumer_offsets-45 broker=1] __consumer_offsets-45 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,301] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,301] INFO [Partition __consumer_offsets-26 broker=1] __consumer_offsets-26 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,341] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,342] INFO [Partition __consumer_offsets-7 broker=1] __consumer_offsets-7 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,367] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,367] INFO [Partition __consumer_offsets-42 broker=1] __consumer_offsets-42 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,388] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,389] INFO [Partition __consumer_offsets-4 broker=1] __consumer_offsets-4 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,413] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,415] INFO [Partition __consumer_offsets-23 broker=1] __consumer_offsets-23 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,474] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,475] INFO [Partition __consumer_offsets-1 broker=1] __consumer_offsets-1 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,498] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,499] INFO [Partition __consumer_offsets-20 broker=1] __consumer_offsets-20 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,529] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,532] INFO [Partition __consumer_offsets-39 broker=1] __consumer_offsets-39 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,556] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,557] INFO [Partition __consumer_offsets-17 broker=1] __consumer_offsets-17 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,581] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,581] INFO [Partition __consumer_offsets-36 broker=1] __consumer_offsets-36 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,638] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,638] INFO [Partition __consumer_offsets-14 broker=1] __consumer_offsets-14 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,672] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,672] INFO [Partition __consumer_offsets-33 broker=1] __consumer_offsets-33 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,697] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,697] INFO [Partition __consumer_offsets-49 broker=1] __consumer_offsets-49 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,728] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,728] INFO [Partition __consumer_offsets-11 broker=1] __consumer_offsets-11 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,754] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,755] INFO [Partition __consumer_offsets-30 broker=1] __consumer_offsets-30 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,781] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,781] INFO [Partition __consumer_offsets-46 broker=1] __consumer_offsets-46 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,806] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,806] INFO [Partition __consumer_offsets-27 broker=1] __consumer_offsets-27 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,836] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,836] INFO [Partition __consumer_offsets-8 broker=1] __consumer_offsets-8 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,866] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,869] INFO [Partition __consumer_offsets-24 broker=1] __consumer_offsets-24 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,925] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,926] INFO [Partition __consumer_offsets-43 broker=1] __consumer_offsets-43 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,950] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,951] INFO [Partition __consumer_offsets-5 broker=1] __consumer_offsets-5 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:49,975] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:49,975] INFO [Partition __consumer_offsets-21 broker=1] __consumer_offsets-21 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,000] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,001] INFO [Partition __consumer_offsets-2 broker=1] __consumer_offsets-2 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,025] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,026] INFO [Partition __consumer_offsets-40 broker=1] __consumer_offsets-40 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,050] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,051] INFO [Partition __consumer_offsets-37 broker=1] __consumer_offsets-37 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,077] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,077] INFO [Partition __consumer_offsets-18 broker=1] __consumer_offsets-18 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,106] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,107] INFO [Partition __consumer_offsets-34 broker=1] __consumer_offsets-34 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,114] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:50,142] INFO Result of znode creation at /brokers/ids/2 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:50,144] INFO Registered broker 2 at path /brokers/ids/2 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:50,179] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,185] INFO [Partition __consumer_offsets-15 broker=1] __consumer_offsets-15 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,218] INFO Replica loaded for partition test-cluster-topic1-0 with initial high watermark 42 (kafka.cluster.Replica)
[2019-02-19 21:10:50,224] INFO [Partition test-cluster-topic1-0 broker=1] test-cluster-topic1-0 starts at Leader Epoch 1 from offset 42. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,227] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 241 (kafka.cluster.Replica)
[2019-02-19 21:10:50,233] INFO [Partition __consumer_offsets-12 broker=1] __consumer_offsets-12 starts at Leader Epoch 1 from offset 242. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,232] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:50,233] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:50,232] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:50,245] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,247] INFO [Partition __consumer_offsets-31 broker=1] __consumer_offsets-31 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,260] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:50,261] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:50,265] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,272] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,272] INFO [Partition __consumer_offsets-9 broker=1] __consumer_offsets-9 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,294] INFO [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:6000,blockEndProducerId:6999) by writing to Zk with path version 7 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 21:10:50,314] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:50,317] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 21:10:50,318] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:50,329] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,329] INFO [Partition __consumer_offsets-47 broker=1] __consumer_offsets-47 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,354] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,355] INFO [Partition __consumer_offsets-19 broker=1] __consumer_offsets-19 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,359] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 21:10:50,376] INFO [SocketServer brokerId=2] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 21:10:50,380] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:50,383] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,381] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:50,386] INFO [Partition __consumer_offsets-28 broker=1] __consumer_offsets-28 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,390] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
[2019-02-19 21:10:50,464] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,464] INFO [Partition __consumer_offsets-38 broker=1] __consumer_offsets-38 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,530] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,530] INFO [Partition __consumer_offsets-35 broker=1] __consumer_offsets-35 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,555] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,555] INFO [Partition __consumer_offsets-44 broker=1] __consumer_offsets-44 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,580] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,596] INFO [Partition __consumer_offsets-6 broker=1] __consumer_offsets-6 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,640] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,640] INFO [Partition __consumer_offsets-25 broker=1] __consumer_offsets-25 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,673] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,676] INFO [Partition __consumer_offsets-16 broker=1] __consumer_offsets-16 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,716] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,718] INFO [Partition __consumer_offsets-22 broker=1] __consumer_offsets-22 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,748] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,748] INFO [Partition __consumer_offsets-41 broker=1] __consumer_offsets-41 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,780] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,780] INFO [Partition __consumer_offsets-32 broker=1] __consumer_offsets-32 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,805] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,806] INFO [Partition __consumer_offsets-3 broker=1] __consumer_offsets-3 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,831] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 21:10:50,831] INFO [Partition __consumer_offsets-13 broker=1] __consumer_offsets-13 starts at Leader Epoch 1 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 21:10:50,867] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,875] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,877] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,877] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,878] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,878] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,879] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,879] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,880] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,883] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,885] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,886] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,886] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,885] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 10 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,887] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,889] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,889] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,889] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,890] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,890] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,891] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,891] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,895] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,895] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,897] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,897] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,897] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,897] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,897] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,898] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,898] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,898] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,898] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,899] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,899] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,899] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,899] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,900] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,900] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,900] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,900] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,901] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,901] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,902] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,902] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,901] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,902] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,903] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,903] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,905] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,905] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,906] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,906] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,906] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,907] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,907] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,907] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,907] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,908] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,908] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,908] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,909] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,909] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,909] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,909] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,909] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,910] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,910] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,910] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,910] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,911] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,911] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,911] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,911] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,911] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,912] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,912] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,912] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,912] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,912] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,913] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,913] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,913] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,914] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,916] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,916] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,918] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,956] INFO [GroupCoordinator 1]: Loading group metadata for test-group with generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:50,960] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 40 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,961] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,961] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,962] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,962] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,962] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,962] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,963] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,963] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,963] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,963] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,964] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:50,964] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:51,633] INFO Creating /brokers/ids/3 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:51,653] INFO Result of znode creation at /brokers/ids/3 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:51,655] INFO Registered broker 3 at path /brokers/ids/3 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9093,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 21:10:51,713] INFO [ExpirationReaper-3-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:51,726] INFO [ExpirationReaper-3-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:51,727] INFO [ExpirationReaper-3-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:10:51,751] INFO [GroupCoordinator 3]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:51,752] INFO [GroupCoordinator 3]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:10:51,758] INFO [GroupMetadataManager brokerId=3] Removed 0 expired offsets in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:10:51,787] INFO [ProducerId Manager 3]: Acquired new producerId block (brokerId:3,blockStartProducerId:7000,blockEndProducerId:7999) by writing to Zk with path version 8 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 21:10:51,809] INFO [TransactionCoordinator id=3] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:51,811] INFO [Transaction Marker Channel Manager 3]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 21:10:51,813] INFO [TransactionCoordinator id=3] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:10:51,852] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 21:10:51,877] INFO [SocketServer brokerId=3] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 21:10:51,884] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:51,884] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:10:51,886] INFO [KafkaServer id=3] started (kafka.server.KafkaServer)
[2019-02-19 21:11:00,966] INFO [GroupCoordinator 1]: Member consumer-1-fe9a192c-a3c3-4040-b4d6-2097dc84f922 in group test-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:11:00,971] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 5 (__consumer_offsets-12) (reason: removing member consumer-1-fe9a192c-a3c3-4040-b4d6-2097dc84f922 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:11:00,975] INFO [GroupCoordinator 1]: Member consumer-1-b89109c0-c2de-4923-a8b4-3c6553942e97 in group test-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:11:00,978] INFO [GroupCoordinator 1]: Group test-group with generation 6 is now empty (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:17,805] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 6 (__consumer_offsets-12) (reason: Adding new member consumer-1-6bc5ae34-7f2b-4f29-964f-d838e8ce0c10) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:17,810] INFO [GroupCoordinator 1]: Stabilized group test-group generation 7 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:17,821] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:35,776] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 7 (__consumer_offsets-12) (reason: Adding new member consumer-1-50c646dc-3405-4e33-a798-3103407d2d6c) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:35,835] INFO [GroupCoordinator 1]: Stabilized group test-group generation 8 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:35,837] INFO [GroupCoordinator 1]: Assignment received from leader for group test-group for generation 8 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:12:55,121] WARN Exception causing close of session 0x1000abb47650002: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:12:55,122] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:60856 which had sessionid 0x1000abb47650002 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:13:01,703] INFO Expiring session 0x1000abb47650002, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:13:01,703] INFO Processed session termination for sessionid: 0x1000abb47650002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:06,185] WARN Exception causing close of session 0x1000abb47650001: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:13:06,186] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:60841 which had sessionid 0x1000abb47650001 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:13:10,703] INFO Expiring session 0x1000abb47650001, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:13:10,703] INFO Processed session termination for sessionid: 0x1000abb47650001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:40,519] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 21:13:41,013] INFO starting (kafka.server.KafkaServer)
[2019-02-19 21:13:41,015] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 21:13:41,033] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:13:45,561] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,562] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,562] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,562] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,563] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,563] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,565] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,566] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,566] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,570] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,570] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,571] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,576] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,576] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,577] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,580] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:13:45,615] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:13:45,615] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:13:45,618] INFO Accepted socket connection from /127.0.0.1:57299 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:13:45,618] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:13:45,625] INFO Client attempting to establish new session at /127.0.0.1:57299 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:13:45,654] INFO Established session 0x1000abb47650003 with negotiated timeout 6000 for client /127.0.0.1:57299 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:13:45,656] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000abb47650003, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:13:45,661] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:13:45,705] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x1 zxid:0x147 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,728] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x2 zxid:0x148 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,746] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x3 zxid:0x149 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,763] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x4 zxid:0x14a txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,788] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x5 zxid:0x14b txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,805] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x6 zxid:0x14c txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,820] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x7 zxid:0x14d txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,838] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x8 zxid:0x14e txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,853] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0x9 zxid:0x14f txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,871] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0xa zxid:0x150 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,895] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0xb zxid:0x151 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,912] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0xc zxid:0x152 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:45,930] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650003 type:create cxid:0xd zxid:0x153 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:13:46,121] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 21:13:46,198] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:13:46,208] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 2
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka2-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:13:46,236] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:13:46,236] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:13:46,240] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:13:46,275] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 21:13:46,285] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-02-19 21:13:46,313] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 21:13:46,318] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 21:13:46,579] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 21:13:46,609] INFO [SocketServer brokerId=2] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 21:13:46,631] INFO [ExpirationReaper-2-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:46,634] INFO [ExpirationReaper-2-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:46,634] INFO [ExpirationReaper-2-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:46,646] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 21:13:51,236] INFO Creating /brokers/ids/2 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 21:13:51,264] INFO Result of znode creation at /brokers/ids/2 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 21:13:51,270] INFO Registered broker 2 at path /brokers/ids/2 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 21:13:51,336] INFO [ExpirationReaper-2-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:51,336] INFO [ExpirationReaper-2-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:51,336] INFO [ExpirationReaper-2-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:13:51,358] INFO [GroupCoordinator 2]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:13:51,359] INFO [GroupCoordinator 2]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:13:51,364] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:13:51,400] INFO [ProducerId Manager 2]: Acquired new producerId block (brokerId:2,blockStartProducerId:8000,blockEndProducerId:8999) by writing to Zk with path version 9 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 21:13:51,420] INFO [TransactionCoordinator id=2] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:13:51,422] INFO [Transaction Marker Channel Manager 2]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 21:13:51,422] INFO [TransactionCoordinator id=2] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:13:51,461] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 21:13:51,483] INFO [SocketServer brokerId=2] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 21:13:51,490] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:13:51,490] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:13:51,492] INFO [KafkaServer id=2] started (kafka.server.KafkaServer)
[2019-02-19 21:20:48,843] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 6 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:23:51,360] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:30:48,836] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:33:51,359] INFO [GroupMetadataManager brokerId=2] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:37:16,039] INFO [GroupCoordinator 1]: Member consumer-1-50c646dc-3405-4e33-a798-3103407d2d6c in group test-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:37:16,040] INFO [GroupCoordinator 1]: Preparing to rebalance group test-group in state PreparingRebalance with old generation 8 (__consumer_offsets-12) (reason: removing member consumer-1-50c646dc-3405-4e33-a798-3103407d2d6c on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:37:16,704] WARN Exception causing close of session 0x1000abb47650003: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:37:16,705] INFO Closed socket connection for client /127.0.0.1:57299 which had sessionid 0x1000abb47650003 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:37:18,936] WARN Exception causing close of session 0x1000abb47650000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:37:18,936] INFO Closed socket connection for client /127.0.0.1:60820 which had sessionid 0x1000abb47650000 (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-19 21:37:22,703] INFO Expiring session 0x1000abb47650003, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:37:22,703] INFO Processed session termination for sessionid: 0x1000abb47650003 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:37:25,702] INFO Expiring session 0x1000abb47650000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:37:25,702] INFO Processed session termination for sessionid: 0x1000abb47650000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:46,283] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 21:38:46,763] INFO starting (kafka.server.KafkaServer)
[2019-02-19 21:38:46,764] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 21:38:46,782] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:38:51,320] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,321] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,321] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,321] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,322] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,322] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,323] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,324] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,324] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,325] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,326] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,326] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,327] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,327] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,328] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,331] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 21:38:51,358] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:38:51,358] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:38:51,361] INFO Accepted socket connection from /127.0.0.1:57425 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 21:38:51,361] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:38:51,364] INFO Client attempting to establish new session at /127.0.0.1:57425 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:38:51,392] INFO Established session 0x1000abb47650004 with negotiated timeout 6000 for client /127.0.0.1:57425 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 21:38:51,394] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1000abb47650004, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 21:38:51,399] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 21:38:51,443] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x1 zxid:0x159 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,467] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x2 zxid:0x15a txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,484] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x3 zxid:0x15b txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,501] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x4 zxid:0x15c txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,517] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x5 zxid:0x15d txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,534] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x6 zxid:0x15e txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,550] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x7 zxid:0x15f txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,568] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x8 zxid:0x160 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,583] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0x9 zxid:0x161 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,601] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0xa zxid:0x162 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,617] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0xb zxid:0x163 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,635] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0xc zxid:0x164 txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,651] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:create cxid:0xd zxid:0x165 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:51,829] INFO Cluster ID = by1VU3UUQk6pyB5x23KqzA (kafka.server.KafkaServer)
[2019-02-19 21:38:51,833] WARN No meta.properties file under dir D:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 21:38:51,901] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:38:51,913] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 21:38:51,939] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:38:51,939] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:38:51,942] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 21:38:51,971] INFO Log directory D:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-02-19 21:38:51,978] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 21:38:51,987] INFO Logs loading complete in 9 ms. (kafka.log.LogManager)
[2019-02-19 21:38:52,002] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 21:38:52,006] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 21:38:52,256] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 21:38:52,287] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 21:38:52,308] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:52,309] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:52,309] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:52,322] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 21:38:56,920] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 21:38:56,945] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 21:38:56,948] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 21:38:56,950] WARN No meta.properties file under dir D:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 21:38:57,050] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:57,054] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:57,054] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 21:38:57,095] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:38:57,098] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 21:38:57,103] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:38:57,132] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:9000,blockEndProducerId:9999) by writing to Zk with path version 10 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 21:38:57,163] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:38:57,168] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 21:38:57,168] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 21:38:57,233] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 21:38:57,246] INFO [SocketServer brokerId=0] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 21:38:57,255] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:38:57,308] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 21:38:57,310] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-02-19 21:38:57,362] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:multi cxid:0xcb zxid:0x169 txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:38:57,384] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:multi cxid:0xcd zxid:0x16a txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 21:48:57,098] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 21:58:57,098] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:06:26,650] INFO Got user-level KeeperException when processing sessionid:0x1000abb47650004 type:setData cxid:0xd5 zxid:0x16b txntype:-1 reqpath:n/a Error Path:/config/topics/connect-test Error:KeeperErrorCode = NoNode for /config/topics/connect-test (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:06:26,699] INFO Topic creation Map(connect-test-0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-02-19 22:06:26,751] INFO [KafkaApi-0] Auto creation of topic connect-test with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-19 22:06:26,873] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(connect-test-0) (kafka.server.ReplicaFetcherManager)
[2019-02-19 22:06:26,922] INFO [Log partition=connect-test-0, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:06:26,931] INFO [Log partition=connect-test-0, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 31 ms (kafka.log.Log)
[2019-02-19 22:06:26,933] INFO Created log for partition connect-test-0 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:06:26,934] INFO [Partition connect-test-0 broker=0] No checkpointed highwatermark is found for partition connect-test-0 (kafka.cluster.Partition)
[2019-02-19 22:06:26,936] INFO Replica loaded for partition connect-test-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:06:26,939] INFO [Partition connect-test-0 broker=0] connect-test-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:08:57,099] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:14:43,465] WARN Client session timed out, have not heard from server in 4000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:43,465] INFO Client session timed out, have not heard from server in 4000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:43,573] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:14:43,574] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:14:44,858] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:44,859] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:50,859] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:50,859] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:50,961] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:14:52,791] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:52,792] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:58,793] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:14:58,793] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:00,410] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:00,411] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:06,412] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:06,412] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:08,043] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:08,044] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:14,045] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:14,046] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:15,183] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:15,188] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:21,189] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:21,189] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:21,291] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:15:22,578] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:22,578] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:28,579] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:28,579] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:30,020] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:30,021] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:36,022] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:36,022] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:37,915] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:37,916] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:43,917] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:43,917] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:44,019] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:15:45,212] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:45,213] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:51,214] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:51,214] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:52,847] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:52,848] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:58,848] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:15:58,848] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:00,292] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:00,293] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:06,293] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:06,293] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:07,750] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:07,751] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:13,753] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:13,753] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:15,622] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:15,623] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:21,623] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:21,623] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:21,724] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:16:23,498] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:23,499] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:29,499] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:29,499] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:31,185] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:31,186] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:37,186] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:37,186] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:39,200] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:39,200] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:45,201] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:45,201] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:45,302] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:16:47,210] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:47,211] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:53,211] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:53,211] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:54,896] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:16:54,897] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:00,898] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:00,898] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:02,525] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:02,525] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:08,526] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:08,526] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:09,664] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:09,665] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:15,666] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:15,666] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:16,904] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:16,905] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:22,905] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:22,905] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:24,307] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:24,308] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:30,308] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:30,308] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:31,622] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:31,623] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:37,623] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:37,624] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:39,715] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:39,715] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:45,717] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:45,717] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:47,215] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:47,215] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:53,216] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:53,216] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:53,318] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:17:54,696] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:17:54,697] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:00,697] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:00,697] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:02,737] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:02,738] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:08,737] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:08,737] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:09,996] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:09,997] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:15,998] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:15,998] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:17,243] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:17,244] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:23,244] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:23,244] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:23,346] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:18:24,675] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:24,676] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:30,677] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:30,677] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:32,363] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:32,364] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:38,364] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:38,364] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:40,397] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:40,398] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:46,399] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:46,399] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:46,502] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:18:47,887] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:47,887] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:53,888] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:53,888] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:55,560] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:55,561] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:18:57,098] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:19:01,561] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:01,561] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:03,532] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:03,533] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:09,534] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:09,535] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:10,824] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:10,825] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:16,826] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:16,827] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:18,735] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:18,735] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:24,735] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:24,735] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:25,987] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:25,987] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:31,988] WARN Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:31,988] INFO Client session timed out, have not heard from server in 6001ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:34,031] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:34,033] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:40,033] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:40,033] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:41,995] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:41,996] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:47,996] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:47,996] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:49,904] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:49,907] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:55,908] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:55,908] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:57,496] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:19:57,497] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:03,498] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:03,498] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:04,687] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:04,688] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:10,689] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:10,690] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:11,793] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:11,794] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:17,794] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:17,794] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:19,460] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:19,461] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:25,462] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:25,462] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:27,109] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:27,110] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:33,111] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:33,111] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:34,395] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:34,396] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:40,397] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:40,397] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:42,039] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:42,040] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:48,041] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:48,041] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:49,373] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:49,374] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:55,375] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:55,375] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:56,530] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:20:56,530] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:02,531] WARN Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:02,532] INFO Client session timed out, have not heard from server in 6000ms for sessionid 0x1000abb47650004, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:04,423] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:05,428] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:06,930] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:07,933] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:09,629] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:10,637] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:12,263] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:13,266] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:14,731] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:15,734] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:17,556] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:18,557] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:20,334] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:21,334] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:22,659] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:23,659] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:24,938] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:25,938] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:27,405] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:28,406] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:29,989] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:30,990] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:32,453] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:33,455] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:35,414] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:36,414] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:37,557] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:38,557] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:39,912] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:40,922] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:42,651] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:43,656] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:45,492] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:46,494] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:47,972] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:48,972] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:50,114] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:51,116] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:52,253] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:53,255] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:54,466] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:55,467] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:57,069] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:21:58,070] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:00,064] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:01,066] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:02,941] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:03,943] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:05,432] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:06,436] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:07,546] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:08,548] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:10,039] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:11,040] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:13,111] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:14,112] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:15,632] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:16,634] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:18,285] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:19,287] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:21,188] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:22,190] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:24,158] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:25,159] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:26,448] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:27,449] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:28,632] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:29,634] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:31,113] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:32,116] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:33,744] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:34,746] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:36,262] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:37,262] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:38,494] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:39,497] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:41,169] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:42,170] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:43,633] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:44,637] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:45,884] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:46,885] INFO Socket error occurred: localhost/127.0.0.1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:48,760] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:49,762] INFO Socket error occurred: localhost/0:0:0:0:0:0:0:1:2181: Connection refused: no further information (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:22:51,321] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:23:21,290] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 22:23:21,292] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 22:23:21,292] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 22:23:21,292] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2019-02-19 22:23:21,292] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2019-02-19 22:23:21,305] INFO Reading configuration from: config\zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2019-02-19 22:23:21,305] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2019-02-19 22:23:25,850] INFO Server environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,850] INFO Server environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,851] INFO Server environment:java.version=1.8.0_201 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,851] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,851] INFO Server environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,852] INFO Server environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,853] INFO Server environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,854] INFO Server environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,854] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,854] INFO Server environment:os.name=Windows 10 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,855] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,855] INFO Server environment:os.version=10.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,857] INFO Server environment:user.name=Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,857] INFO Server environment:user.home=C:\Users\Administrator (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,858] INFO Server environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,873] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,873] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,875] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:23:25,903] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[2019-02-19 22:23:25,907] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 22:23:57,671] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2019-02-19 22:23:58,155] INFO starting (kafka.server.KafkaServer)
[2019-02-19 22:23:58,157] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2019-02-19 22:23:58,175] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:24:02,708] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,709] INFO Client environment:host.name=DESKTOP-6IV0LP1 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,709] INFO Client environment:java.version=1.8.0_201 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,709] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,709] INFO Client environment:java.home=C:\Program Files\Java\jdk1.8.0_201\jre (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,709] INFO Client environment:java.class.path=D:\Software\kafka_2.12-2.1.0\libs\activation-1.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\aopalliance-repackaged-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\argparse4j-0.7.0.jar;D:\Software\kafka_2.12-2.1.0\libs\audience-annotations-0.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\commons-lang3-3.5.jar;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping;D:\Software\kafka_2.12-2.1.0\libs\compileScala.mapping.asc;D:\Software\kafka_2.12-2.1.0\libs\connect-api-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-basic-auth-extension-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-file-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-json-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-runtime-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\connect-transforms-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\guava-20.0.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-api-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-locator-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\hk2-utils-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-core-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-databind-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-base-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-jaxrs-json-provider-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\jackson-module-jaxb-annotations-2.9.7.jar;D:\Software\kafka_2.12-2.1.0\libs\javassist-3.22.0-CR2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.annotation-api-1.2.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.inject-2.5.0-b42.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.servlet-api-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.1.jar;D:\Software\kafka_2.12-2.1.0\libs\javax.ws.rs-api-2.1.jar;D:\Software\kafka_2.12-2.1.0\libs\jaxb-api-2.3.0.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-client-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-common-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-container-servlet-core-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-hk2-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-media-jaxb-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jersey-server-2.27.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-client-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-continuation-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-http-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-io-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-security-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-server-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlet-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-servlets-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jetty-util-9.4.12.v20180830.jar;D:\Software\kafka_2.12-2.1.0\libs\jopt-simple-5.0.4.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-clients-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-log4j-appender-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-examples-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-scala_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-streams-test-utils-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka-tools-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-javadoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-scaladoc.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test-sources.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0-test.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\kafka_2.12-2.1.0.jar.asc;D:\Software\kafka_2.12-2.1.0\libs\log4j-1.2.17.jar;D:\Software\kafka_2.12-2.1.0\libs\lz4-java-1.5.0.jar;D:\Software\kafka_2.12-2.1.0\libs\maven-artifact-3.5.4.jar;D:\Software\kafka_2.12-2.1.0\libs\metrics-core-2.2.0.jar;D:\Software\kafka_2.12-2.1.0\libs\osgi-resource-locator-1.0.1.jar;D:\Software\kafka_2.12-2.1.0\libs\plexus-utils-3.1.0.jar;D:\Software\kafka_2.12-2.1.0\libs\reflections-0.9.11.jar;D:\Software\kafka_2.12-2.1.0\libs\rocksdbjni-5.14.2.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-library-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-logging_2.12-3.9.0.jar;D:\Software\kafka_2.12-2.1.0\libs\scala-reflect-2.12.7.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-api-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\slf4j-log4j12-1.7.25.jar;D:\Software\kafka_2.12-2.1.0\libs\snappy-java-1.1.7.2.jar;D:\Software\kafka_2.12-2.1.0\libs\validation-api-1.1.0.Final.jar;D:\Software\kafka_2.12-2.1.0\libs\zkclient-0.10.jar;D:\Software\kafka_2.12-2.1.0\libs\zookeeper-3.4.13.jar;D:\Software\kafka_2.12-2.1.0\libs\zstd-jni-1.3.5-4.jar (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,710] INFO Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_201\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;%JAVA_HOME%\bin;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Windows\System32\OpenSSH\;C:\Users\Administrator\AppData\Local\Microsoft\WindowsApps;;. (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,711] INFO Client environment:java.io.tmpdir=C:\Users\ADMINI~1\AppData\Local\Temp\ (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,711] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,711] INFO Client environment:os.name=Windows 10 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,712] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,712] INFO Client environment:os.version=10.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,712] INFO Client environment:user.name=Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,712] INFO Client environment:user.home=C:\Users\Administrator (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,713] INFO Client environment:user.dir=D:\Software\kafka_2.12-2.1.0 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,714] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@78b66d36 (org.apache.zookeeper.ZooKeeper)
[2019-02-19 22:24:02,731] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:24:02,732] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:24:02,734] INFO Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:24:02,734] INFO Accepted socket connection from /0:0:0:0:0:0:0:1:57990 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2019-02-19 22:24:02,740] INFO Client attempting to establish new session at /0:0:0:0:0:0:0:1:57990 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:24:02,742] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2019-02-19 22:24:02,786] INFO Established session 0x1000afdf7e30000 with negotiated timeout 6000 for client /0:0:0:0:0:0:0:1:57990 (org.apache.zookeeper.server.ZooKeeperServer)
[2019-02-19 22:24:02,788] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000afdf7e30000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2019-02-19 22:24:02,792] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2019-02-19 22:24:02,865] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:02,915] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:02,953] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:03,277] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:03,314] INFO Cluster ID = URXgyl9wTlKBvAlV2QSnWA (kafka.server.KafkaServer)
[2019-02-19 22:24:03,324] WARN No meta.properties file under dir D:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 22:24:03,387] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 22:24:03,396] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 2.1-IV2
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.1-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2019-02-19 22:24:03,421] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 22:24:03,421] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 22:24:03,423] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2019-02-19 22:24:03,447] INFO Log directory D:\tmp\kafka-logs not found, creating it. (kafka.log.LogManager)
[2019-02-19 22:24:03,454] INFO Loading logs. (kafka.log.LogManager)
[2019-02-19 22:24:03,464] INFO Logs loading complete in 8 ms. (kafka.log.LogManager)
[2019-02-19 22:24:03,476] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2019-02-19 22:24:03,481] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2019-02-19 22:24:03,736] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2019-02-19 22:24:03,767] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2019-02-19 22:24:03,791] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:03,792] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:03,792] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:03,806] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2019-02-19 22:24:08,369] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2019-02-19 22:24:08,401] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2019-02-19 22:24:08,403] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(DESKTOP-6IV0LP1,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2019-02-19 22:24:08,406] WARN No meta.properties file under dir D:\tmp\kafka-logs\meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2019-02-19 22:24:08,507] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:08,512] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:08,512] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2019-02-19 22:24:08,541] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[2019-02-19 22:24:08,548] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:08,549] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:08,554] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:08,598] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2019-02-19 22:24:08,620] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 22:24:08,624] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2019-02-19 22:24:08,626] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2019-02-19 22:24:08,686] INFO [SocketServer brokerId=0] Started processors for 1 acceptors (kafka.network.SocketServer)
[2019-02-19 22:24:08,702] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2019-02-19 22:24:08,709] INFO Kafka version : 2.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 22:24:08,726] INFO Kafka commitId : 809be928f1ae004e (org.apache.kafka.common.utils.AppInfoParser)
[2019-02-19 22:24:08,727] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:multi cxid:0x36 zxid:0x1c txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:08,728] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2019-02-19 22:24:08,755] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:multi cxid:0x38 zxid:0x1d txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:25,833] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:setData cxid:0x40 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/test-file-topic Error:KeeperErrorCode = NoNode for /config/topics/test-file-topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:25,865] INFO Topic creation Map(test-file-topic-0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-02-19 22:24:25,889] INFO [KafkaApi-0] Auto creation of topic test-file-topic with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-19 22:24:25,910] INFO Got user-level KeeperException when processing sessionid:0x1000afdf7e30000 type:setData cxid:0x49 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2019-02-19 22:24:25,948] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(0), __consumer_offsets-30 -> ArrayBuffer(0), __consumer_offsets-8 -> ArrayBuffer(0), __consumer_offsets-21 -> ArrayBuffer(0), __consumer_offsets-4 -> ArrayBuffer(0), __consumer_offsets-27 -> ArrayBuffer(0), __consumer_offsets-7 -> ArrayBuffer(0), __consumer_offsets-9 -> ArrayBuffer(0), __consumer_offsets-46 -> ArrayBuffer(0), __consumer_offsets-25 -> ArrayBuffer(0), __consumer_offsets-35 -> ArrayBuffer(0), __consumer_offsets-41 -> ArrayBuffer(0), __consumer_offsets-33 -> ArrayBuffer(0), __consumer_offsets-23 -> ArrayBuffer(0), __consumer_offsets-49 -> ArrayBuffer(0), __consumer_offsets-47 -> ArrayBuffer(0), __consumer_offsets-16 -> ArrayBuffer(0), __consumer_offsets-28 -> ArrayBuffer(0), __consumer_offsets-31 -> ArrayBuffer(0), __consumer_offsets-36 -> ArrayBuffer(0), __consumer_offsets-42 -> ArrayBuffer(0), __consumer_offsets-3 -> ArrayBuffer(0), __consumer_offsets-18 -> ArrayBuffer(0), __consumer_offsets-37 -> ArrayBuffer(0), __consumer_offsets-15 -> ArrayBuffer(0), __consumer_offsets-24 -> ArrayBuffer(0), __consumer_offsets-38 -> ArrayBuffer(0), __consumer_offsets-17 -> ArrayBuffer(0), __consumer_offsets-48 -> ArrayBuffer(0), __consumer_offsets-19 -> ArrayBuffer(0), __consumer_offsets-11 -> ArrayBuffer(0), __consumer_offsets-13 -> ArrayBuffer(0), __consumer_offsets-2 -> ArrayBuffer(0), __consumer_offsets-43 -> ArrayBuffer(0), __consumer_offsets-6 -> ArrayBuffer(0), __consumer_offsets-14 -> ArrayBuffer(0), __consumer_offsets-20 -> ArrayBuffer(0), __consumer_offsets-0 -> ArrayBuffer(0), __consumer_offsets-44 -> ArrayBuffer(0), __consumer_offsets-39 -> ArrayBuffer(0), __consumer_offsets-12 -> ArrayBuffer(0), __consumer_offsets-45 -> ArrayBuffer(0), __consumer_offsets-1 -> ArrayBuffer(0), __consumer_offsets-5 -> ArrayBuffer(0), __consumer_offsets-26 -> ArrayBuffer(0), __consumer_offsets-29 -> ArrayBuffer(0), __consumer_offsets-34 -> ArrayBuffer(0), __consumer_offsets-10 -> ArrayBuffer(0), __consumer_offsets-32 -> ArrayBuffer(0), __consumer_offsets-40 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2019-02-19 22:24:25,978] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2019-02-19 22:24:26,019] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(test-file-topic-0) (kafka.server.ReplicaFetcherManager)
[2019-02-19 22:24:26,083] INFO [Log partition=test-file-topic-0, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,094] INFO [Log partition=test-file-topic-0, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2019-02-19 22:24:26,096] INFO Created log for partition test-file-topic-0 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,098] INFO [Partition test-file-topic-0 broker=0] No checkpointed highwatermark is found for partition test-file-topic-0 (kafka.cluster.Partition)
[2019-02-19 22:24:26,100] INFO Replica loaded for partition test-file-topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,106] INFO [Partition test-file-topic-0 broker=0] test-file-topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,548] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[2019-02-19 22:24:26,553] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,554] INFO [Log partition=__consumer_offsets-0, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:26,555] INFO Created log for partition __consumer_offsets-0 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,556] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2019-02-19 22:24:26,557] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,557] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,599] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,600] INFO [Log partition=__consumer_offsets-29, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:26,601] INFO Created log for partition __consumer_offsets-29 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,602] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2019-02-19 22:24:26,602] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,602] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,639] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,640] INFO [Log partition=__consumer_offsets-48, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:26,641] INFO Created log for partition __consumer_offsets-48 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,643] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2019-02-19 22:24:26,643] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,644] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,681] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,682] INFO [Log partition=__consumer_offsets-10, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:26,683] INFO Created log for partition __consumer_offsets-10 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,684] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2019-02-19 22:24:26,685] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,685] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,727] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,729] INFO [Log partition=__consumer_offsets-45, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:26,731] INFO Created log for partition __consumer_offsets-45 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,732] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2019-02-19 22:24:26,732] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,733] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,768] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,770] INFO [Log partition=__consumer_offsets-26, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:26,772] INFO Created log for partition __consumer_offsets-26 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,773] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2019-02-19 22:24:26,773] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,775] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,811] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,812] INFO [Log partition=__consumer_offsets-7, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:26,813] INFO Created log for partition __consumer_offsets-7 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,814] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2019-02-19 22:24:26,814] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,815] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,849] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,850] INFO [Log partition=__consumer_offsets-42, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:26,851] INFO Created log for partition __consumer_offsets-42 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,852] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2019-02-19 22:24:26,853] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,853] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,890] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,891] INFO [Log partition=__consumer_offsets-4, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:26,892] INFO Created log for partition __consumer_offsets-4 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,893] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2019-02-19 22:24:26,894] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,894] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,935] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,937] INFO [Log partition=__consumer_offsets-23, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:26,938] INFO Created log for partition __consumer_offsets-23 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,939] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2019-02-19 22:24:26,940] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,940] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,977] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:26,979] INFO [Log partition=__consumer_offsets-1, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:26,981] INFO Created log for partition __consumer_offsets-1 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:26,982] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2019-02-19 22:24:26,982] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:26,984] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,027] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,029] INFO [Log partition=__consumer_offsets-20, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,031] INFO Created log for partition __consumer_offsets-20 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,032] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2019-02-19 22:24:27,032] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,033] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,077] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,079] INFO [Log partition=__consumer_offsets-39, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,081] INFO Created log for partition __consumer_offsets-39 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,082] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2019-02-19 22:24:27,082] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,084] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,127] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,129] INFO [Log partition=__consumer_offsets-17, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,131] INFO Created log for partition __consumer_offsets-17 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,132] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2019-02-19 22:24:27,132] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,133] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,169] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,170] INFO [Log partition=__consumer_offsets-36, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,172] INFO Created log for partition __consumer_offsets-36 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,173] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2019-02-19 22:24:27,174] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,175] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,211] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,212] INFO [Log partition=__consumer_offsets-14, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,214] INFO Created log for partition __consumer_offsets-14 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,215] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2019-02-19 22:24:27,215] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,215] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,252] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,254] INFO [Log partition=__consumer_offsets-33, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,256] INFO Created log for partition __consumer_offsets-33 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,257] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2019-02-19 22:24:27,257] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,258] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,303] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,305] INFO [Log partition=__consumer_offsets-49, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,307] INFO Created log for partition __consumer_offsets-49 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,308] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2019-02-19 22:24:27,309] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,310] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,353] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,354] INFO [Log partition=__consumer_offsets-11, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,356] INFO Created log for partition __consumer_offsets-11 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,357] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2019-02-19 22:24:27,357] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,358] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,396] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,398] INFO [Log partition=__consumer_offsets-30, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,400] INFO Created log for partition __consumer_offsets-30 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,401] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2019-02-19 22:24:27,402] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,403] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,444] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,446] INFO [Log partition=__consumer_offsets-46, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,448] INFO Created log for partition __consumer_offsets-46 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,449] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2019-02-19 22:24:27,449] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,450] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,502] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,504] INFO [Log partition=__consumer_offsets-27, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:27,505] INFO Created log for partition __consumer_offsets-27 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,506] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2019-02-19 22:24:27,507] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,507] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,540] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,541] INFO [Log partition=__consumer_offsets-8, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,542] INFO Created log for partition __consumer_offsets-8 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,544] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2019-02-19 22:24:27,544] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,545] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,581] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,583] INFO [Log partition=__consumer_offsets-24, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,584] INFO Created log for partition __consumer_offsets-24 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,584] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2019-02-19 22:24:27,585] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,585] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,632] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,633] INFO [Log partition=__consumer_offsets-43, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,634] INFO Created log for partition __consumer_offsets-43 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,636] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2019-02-19 22:24:27,636] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,636] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,674] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,675] INFO [Log partition=__consumer_offsets-5, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:27,676] INFO Created log for partition __consumer_offsets-5 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,678] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2019-02-19 22:24:27,678] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,678] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,715] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,716] INFO [Log partition=__consumer_offsets-21, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,717] INFO Created log for partition __consumer_offsets-21 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,719] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2019-02-19 22:24:27,719] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,719] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,761] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,763] INFO [Log partition=__consumer_offsets-2, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,765] INFO Created log for partition __consumer_offsets-2 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,766] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2019-02-19 22:24:27,766] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,767] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,813] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,814] INFO [Log partition=__consumer_offsets-40, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:27,816] INFO Created log for partition __consumer_offsets-40 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,817] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2019-02-19 22:24:27,818] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,818] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,874] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,877] INFO [Log partition=__consumer_offsets-37, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2019-02-19 22:24:27,879] INFO Created log for partition __consumer_offsets-37 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,880] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2019-02-19 22:24:27,881] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,882] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,915] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,916] INFO [Log partition=__consumer_offsets-18, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,917] INFO Created log for partition __consumer_offsets-18 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,919] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2019-02-19 22:24:27,919] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,919] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:27,957] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:27,958] INFO [Log partition=__consumer_offsets-34, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:27,959] INFO Created log for partition __consumer_offsets-34 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:27,961] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2019-02-19 22:24:27,961] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:27,961] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,015] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,017] INFO [Log partition=__consumer_offsets-15, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:28,018] INFO Created log for partition __consumer_offsets-15 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,020] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2019-02-19 22:24:28,021] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,021] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,065] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,067] INFO [Log partition=__consumer_offsets-12, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:28,069] INFO Created log for partition __consumer_offsets-12 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,070] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2019-02-19 22:24:28,070] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,071] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,111] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,115] INFO [Log partition=__consumer_offsets-31, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2019-02-19 22:24:28,117] INFO Created log for partition __consumer_offsets-31 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,117] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2019-02-19 22:24:28,118] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,118] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,166] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,168] INFO [Log partition=__consumer_offsets-9, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,169] INFO Created log for partition __consumer_offsets-9 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,170] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2019-02-19 22:24:28,170] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,171] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,211] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,214] INFO [Log partition=__consumer_offsets-47, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,219] INFO Created log for partition __consumer_offsets-47 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,220] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2019-02-19 22:24:28,221] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,222] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,273] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,275] INFO [Log partition=__consumer_offsets-19, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:28,276] INFO Created log for partition __consumer_offsets-19 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,278] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2019-02-19 22:24:28,278] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,279] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,324] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,327] INFO [Log partition=__consumer_offsets-28, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,331] INFO Created log for partition __consumer_offsets-28 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,334] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2019-02-19 22:24:28,337] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,337] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,379] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,380] INFO [Log partition=__consumer_offsets-38, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:28,381] INFO Created log for partition __consumer_offsets-38 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,382] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2019-02-19 22:24:28,382] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,383] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,490] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,492] INFO [Log partition=__consumer_offsets-35, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2019-02-19 22:24:28,493] INFO Created log for partition __consumer_offsets-35 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,494] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2019-02-19 22:24:28,494] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,495] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,540] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,542] INFO [Log partition=__consumer_offsets-44, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,544] INFO Created log for partition __consumer_offsets-44 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,544] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2019-02-19 22:24:28,545] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,546] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,603] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,604] INFO [Log partition=__consumer_offsets-6, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:28,605] INFO Created log for partition __consumer_offsets-6 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,606] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2019-02-19 22:24:28,607] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,607] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,645] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,646] INFO [Log partition=__consumer_offsets-25, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:28,648] INFO Created log for partition __consumer_offsets-25 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,649] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2019-02-19 22:24:28,649] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,650] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,736] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,737] INFO [Log partition=__consumer_offsets-16, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:28,738] INFO Created log for partition __consumer_offsets-16 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,740] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2019-02-19 22:24:28,740] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,740] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,779] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,780] INFO [Log partition=__consumer_offsets-22, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2019-02-19 22:24:28,780] INFO Created log for partition __consumer_offsets-22 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,782] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2019-02-19 22:24:28,782] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,782] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,823] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,826] INFO [Log partition=__consumer_offsets-41, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,833] INFO Created log for partition __consumer_offsets-41 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,840] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2019-02-19 22:24:28,840] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,842] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,925] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,926] INFO [Log partition=__consumer_offsets-32, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:28,927] INFO Created log for partition __consumer_offsets-32 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,929] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2019-02-19 22:24:28,929] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,929] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:28,970] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:28,971] INFO [Log partition=__consumer_offsets-3, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2019-02-19 22:24:28,972] INFO Created log for partition __consumer_offsets-3 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:28,973] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2019-02-19 22:24:28,973] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:28,974] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:29,070] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[2019-02-19 22:24:29,071] INFO [Log partition=__consumer_offsets-13, dir=D:\tmp\kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2019-02-19 22:24:29,072] INFO Created log for partition __consumer_offsets-13 in D:\tmp\kafka-logs with properties {compression.type -> producer, message.format.version -> 2.1-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2019-02-19 22:24:29,074] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2019-02-19 22:24:29,074] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2019-02-19 22:24:29,074] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2019-02-19 22:24:29,111] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,111] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,112] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,114] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,114] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,114] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,115] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,115] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,116] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,116] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,116] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,117] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,117] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,117] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,117] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,118] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,118] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,118] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,118] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,119] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,119] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,119] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,119] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,120] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,122] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,122] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,123] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,124] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,124] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,124] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,124] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,125] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,125] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,125] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,126] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,126] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,126] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,126] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,127] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,127] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,127] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,127] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,128] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,128] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 17 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,128] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,128] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,128] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,129] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,129] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,129] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,130] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,131] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,136] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,137] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,138] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,138] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,139] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,140] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,140] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,141] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,142] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,143] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,144] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,146] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,148] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,150] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,151] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,151] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,154] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,159] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,161] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,162] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,162] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,163] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,164] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,164] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,164] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,165] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,165] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,166] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,166] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,167] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,168] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 0 (__consumer_offsets-7) (reason: Adding new member consumer-1-d6dabea2-69c8-4f9d-bc4e-610fa385991f) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:29,169] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:24:29,180] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 1 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:29,190] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:49,051] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 1 (__consumer_offsets-7) (reason: removing member consumer-1-d6dabea2-69c8-4f9d-bc4e-610fa385991f on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:24:49,053] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 2 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:25:20,350] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 2 (__consumer_offsets-7) (reason: Adding new member consumer-1-cda692f4-74c8-4850-a866-5ab54dc3ceaa) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:25:20,352] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 3 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:25:20,356] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:27:20,621] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 3 (__consumer_offsets-7) (reason: removing member consumer-1-cda692f4-74c8-4850-a866-5ab54dc3ceaa on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:27:20,622] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 4 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:29:43,336] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 4 (__consumer_offsets-7) (reason: Adding new member consumer-1-b32beb0c-890c-44a6-acb1-ed1800e555be) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:29:43,337] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 5 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:29:43,341] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:30:46,224] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 5 (__consumer_offsets-7) (reason: removing member consumer-1-b32beb0c-890c-44a6-acb1-ed1800e555be on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:30:46,225] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 6 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:30:54,441] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 6 (__consumer_offsets-7) (reason: Adding new member consumer-1-e970bfdd-6029-45c2-b635-a5b46e6e9732) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:30:54,442] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 7 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:30:54,447] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:33:21,706] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 7 (__consumer_offsets-7) (reason: removing member consumer-1-e970bfdd-6029-45c2-b635-a5b46e6e9732 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:33:21,706] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 8 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:33:28,954] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 8 (__consumer_offsets-7) (reason: Adding new member consumer-1-5dbc7a40-2c74-43b8-ba13-6feeb0d08779) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:33:28,955] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 9 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:33:28,959] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 9 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:34:08,554] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:34:44,026] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 9 (__consumer_offsets-7) (reason: removing member consumer-1-5dbc7a40-2c74-43b8-ba13-6feeb0d08779 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:34:44,027] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 10 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:34:50,263] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 10 (__consumer_offsets-7) (reason: Adding new member consumer-1-f9ee4248-9b55-4c7b-9db3-6eec25890dd1) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:34:50,264] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 11 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:34:50,269] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 11 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:35:56,683] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 11 (__consumer_offsets-7) (reason: removing member consumer-1-f9ee4248-9b55-4c7b-9db3-6eec25890dd1 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:35:56,686] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 12 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:36:02,997] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 12 (__consumer_offsets-7) (reason: Adding new member consumer-1-5f2875e0-fa38-424d-b4d2-acdd12d65143) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:36:02,998] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 13 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:36:03,003] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 13 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:39:57,846] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 13 (__consumer_offsets-7) (reason: removing member consumer-1-5f2875e0-fa38-424d-b4d2-acdd12d65143 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:39:57,847] INFO [GroupCoordinator 0]: Group connect-local-file-sink with generation 14 is now empty (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:40:04,929] INFO [GroupCoordinator 0]: Preparing to rebalance group connect-local-file-sink in state PreparingRebalance with old generation 14 (__consumer_offsets-7) (reason: Adding new member consumer-1-258d25a0-2b24-46b5-9492-b816c0abd7f5) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:40:04,931] INFO [GroupCoordinator 0]: Stabilized group connect-local-file-sink generation 15 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:40:04,936] INFO [GroupCoordinator 0]: Assignment received from leader for group connect-local-file-sink for generation 15 (kafka.coordinator.group.GroupCoordinator)
[2019-02-19 22:44:08,549] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 22:54:08,551] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:04:08,549] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:14:08,550] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:24:08,551] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:34:08,550] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:44:08,551] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-19 23:54:08,550] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-20 00:04:08,551] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-20 00:14:08,550] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-20 00:24:08,550] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2019-02-20 00:33:47,998] WARN Exception causing close of session 0x1000afdf7e30000: An existing connection was forcibly closed by the remote host (org.apache.zookeeper.server.NIOServerCnxn)
[2019-02-20 00:33:48,001] INFO Closed socket connection for client /0:0:0:0:0:0:0:1:57990 which had sessionid 0x1000afdf7e30000 (org.apache.zookeeper.server.NIOServerCnxn)
